{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Topo",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNR3b7hwCQ-L"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK3eLTa_EBUg",
        "outputId": "67734481-5da5-4298-b4b9-b7edcc488c16"
      },
      "source": [
        "!pip install py3dep -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 419kB 19.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 46.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5MB 50.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 41.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 58.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 481kB 50.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 56.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7MB 48.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 19.1MB 214kB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 57.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 52.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 38.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.3MB 339kB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 51.2MB/s \n",
            "\u001b[?25h  Building wheel for brotlipy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31VLqbWoECp6"
      },
      "source": [
        "!pip install pynhd -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb6XNyFhEF1v"
      },
      "source": [
        "!pip install geopandas -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug45f8A0LQZu"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VaSylU9h5hY"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import save\n",
        "import torch.utils.tensorboard as tb\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import random \n",
        "import os, math\n",
        "from os import path\n",
        "import seaborn as sns\n",
        "\n",
        "import py3dep\n",
        "import geopandas as gpd\n",
        "from pynhd import NLDI\n",
        "import xarray as xr\n",
        "import rasterio\n",
        "import shapely\n",
        "from shapely.geometry import Point\n",
        "import warnings\n",
        "from shapely.ops import transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bLR4VYHC2q-"
      },
      "source": [
        "# Random Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NldPpbeAi_-5"
      },
      "source": [
        "class SuperTuxDataset(Dataset):\n",
        "    def __init__(self, num_samples, data_transforms=None):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        Hint: Use the python csv library to parse labels.csv\n",
        "        \"\"\"\n",
        "        X = np.zeros((num_samples, 4, 24, 24))\n",
        "        y = np.zeros((1, num_samples))\n",
        "\n",
        "        for i in range(num_samples):\n",
        "          next_y = 0\n",
        "          for j in range(4):\n",
        "            X[i, j, 0, 0] = random.gauss(1,1)\n",
        "            h_slope = random.randint(0,1)\n",
        "            v_slope = random.randint(0,1)\n",
        "            next_y += h_slope + v_slope\n",
        "            if h_slope == 0:\n",
        "              for k in range(23):\n",
        "                X[i, j, 0, k] = X[i, j, 0, k - 1] - random.gauss(1, 0.1)\n",
        "            else:\n",
        "              for k in range(23):\n",
        "                X[i, j, 0, k] = X[i, j, 0, k - 1] + random.gauss(1, 0.1)\n",
        "            \n",
        "            if v_slope == 0:\n",
        "              for l in range(23):\n",
        "                for m in range(23):\n",
        "                  X[i, j, l, m] = X[i, j, l - 1, m] - random.gauss(1, 0.1)\n",
        "            else:\n",
        "              for l in range(23):\n",
        "                for m in range(23):\n",
        "                  X[i, j, l, m] = X[i, j, l - 1, m] + random.gauss(1, 0.1)\n",
        "            y[0, i] = next_y % 2\n",
        "                  \n",
        "        #for i in range(num_samples):\n",
        "        #  for j in range(24):\n",
        "        #    for k in range(24):\n",
        "        #      x = random.gauss(2,1)\n",
        "        #      X[i - 1, 0, j - 1, k - 1] = x + random.gauss(1, 1)\n",
        "        #      X[i - 1, 1, j - 1, k - 1] = 4 * x + random.gauss(1, 1)\n",
        "        #      X[i - 1, 2, j - 1, k - 1] = x * x + random.gauss(1, 1)\n",
        "        #      X[i - 1, 3, j - 1, k - 1] = math.exp(x) + random.gauss(1, 1)\n",
        "        #ave = np.average(np.average(np.average(np.average(X))))\n",
        "\n",
        "        self.files = torch.from_numpy(X)\n",
        "\n",
        "        #y = [np.average(np.average(np.average(X[i - 1]))) < ave for i in range(num_samples)]\n",
        "        #y = np.array(y)\n",
        "        #y = y.astype(int)\n",
        "   \n",
        "        self.labels = y\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        \"\"\"\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        return a tuple: img, label\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "          idx = idx.tolist()\n",
        "        \n",
        "        image = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "        sample = (image,label)\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhd-wCXlnJPE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "5b4eae84-08c1-4e0e-985c-a4ed196d78bb"
      },
      "source": [
        "def test_dataset():\n",
        "  train_dataset = SuperTuxDataset(500)\n",
        "\n",
        "  # Tests for __len__\n",
        "  assert type(len(train_dataset)) == int, f\"The output type for __len__ should be an int. Yours is {type(len(train_dataset))}\"\n",
        "\n",
        "  assert len(train_dataset) == 500, f\"Incorrect number of training examples returned. It should be 21000. Yours is {len(train_dataset)}\"\n",
        "\n",
        "  # Tests for __getitem__\n",
        "  assert type(train_dataset[0]) == tuple, \"Incorrect output type for __getitem_, tuples should be returned\"\n",
        "  assert len(train_dataset[0]) == 2, \"Incorrect output for __getitem__, a tuple containing two elements should be returned\"\n",
        "  \n",
        "  assert train_dataset[0][0].shape == torch.Size([4, 11, 11]) and (train_dataset[0][1] == 0 or train_dataset[0][1] == 1), \"Incorrect output for __getitem__, a tuple containing two elements: (1) a tensor of shape (3,64,64) and (2) a label (int) should be returned\"\n",
        "\n",
        "  print(\"[SUCCESSFUL!] Congrats! Your implementation of dataload looks alright so far! You'll run visualize_data() function below to see if it can return data sample correct!\")\n",
        "\n",
        "test_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-17dc7e4cea96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[SUCCESSFUL!] Congrats! Your implementation of dataload looks alright so far! You'll run visualize_data() function below to see if it can return data sample correct!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-17dc7e4cea96>\u001b[0m in \u001b[0;36mtest_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect output for __getitem__, a tuple containing two elements should be returned\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect output for __getitem__, a tuple containing two elements: (1) a tensor of shape (3,64,64) and (2) a label (int) should be returned\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[SUCCESSFUL!] Congrats! Your implementation of dataload looks alright so far! You'll run visualize_data() function below to see if it can return data sample correct!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Incorrect output for __getitem__, a tuple containing two elements: (1) a tensor of shape (3,64,64) and (2) a label (int) should be returned"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F92q12FgD10N"
      },
      "source": [
        "# Set up DEM Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_B3pV3ZD8E8",
        "outputId": "bba2830b-3245-428f-90d9-4396c88a7851"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRM5o47TPEGf",
        "outputId": "849f6370-6018-4d60-cbe2-f335206a0e39"
      },
      "source": [
        "landslides_path = '/content/drive/MyDrive/Landslide Project/datasets/landslide_geojson.geojson'\n",
        "\n",
        "landslide_gdf = gpd.read_file(landslides_path)\n",
        "print(landslide_gdf.crs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epsg:3857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "IWutf-aaPIU3",
        "outputId": "fdc08338-383b-4ff8-9153-8fc619cecd01"
      },
      "source": [
        "landslide_filtered = landslide_gdf[(landslide_gdf['YEAR'] > 0) & \n",
        "              (landslide_gdf['LENGTH_ft'] > args.length_min) & \n",
        "              (landslide_gdf['WIDTH_ft'] > args.width_min)]\n",
        "\n",
        "len(landslide_filtered)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-91b2fb585815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m landslide_filtered = landslide_gdf[(landslide_gdf['YEAR'] > 0) & \n\u001b[0;32m----> 2\u001b[0;31m               \u001b[0;34m(\u001b[0m\u001b[0mlandslide_gdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LENGTH_ft'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength_min\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m               (landslide_gdf['WIDTH_ft'] > args.width_min)]\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandslide_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQBHcs3wPYKR"
      },
      "source": [
        "def get_no_landslide(lslide_df, layer_types = [\"DEM\"], debugging=False, radius_shift=8):\n",
        "  \"\"\"\n",
        "  lslide_df = GeoDataFrame that contains all the landslide points\n",
        "\n",
        "\n",
        "  layer_types = OPTIONAL (Can select multiple)\n",
        "    Options for layer_type for pulling the right layer type: \n",
        "    \"DEM\"\n",
        "    \"Hillshade Gray\"\n",
        "    \"Aspect Degrees\"\n",
        "    \"Aspect Map\"\n",
        "    \"GreyHillshade_elevationFill\"\n",
        "    \"Hillshade Multidirectional\"\n",
        "    \"Slope Map\"\n",
        "    \"Slope Degrees\"\n",
        "    \"Hillshade Elevation Tinted\"\n",
        "    \"Height Ellipsoidal\"\n",
        "    \"Contour 25\"\n",
        "    \"Contour Smoothed 25\"\n",
        "\n",
        "    debugging = OPTIONAL boolean for having an output\n",
        "\n",
        "    radius_shift = OPTIONAL int for determining the potental random bounds METER \n",
        "    shift of the center\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  min_x = lslide_df['geometry'].x.min()\n",
        "  max_x = lslide_df['geometry'].x.max()\n",
        "\n",
        "  min_y = lslide_df['geometry'].y.min()\n",
        "  max_y = lslide_df['geometry'].y.max()\n",
        "\n",
        "  while(True):\n",
        "    # get a random float from extents of the landslide database\n",
        "    x = random.uniform(min_x, max_x)\n",
        "    y = random.uniform(min_y, max_y)\n",
        "    \n",
        "    if debugging:\n",
        "      \"\"\"\n",
        "      # Test x, y for debugging the contains\n",
        "      x = -13259575.547\n",
        "      y = 5768131.881\n",
        "      \"\"\"\n",
        "      print(f'x: {x} y: {y}')\n",
        "\n",
        "    p = Point(x, y)\n",
        "\n",
        "    # random int between -9 and 9\n",
        "    x_shift = random.randint(radius_shift*-1, radius_shift)\n",
        "    y_shift = random.randint(radius_shift*-1, radius_shift)\n",
        "\n",
        "    lslide_point = gpd.GeoSeries([p])\n",
        "    lslide_point = lslide_point.set_crs(epsg=3857)\n",
        "    \n",
        "    lslide_point = lslide_point.to_crs({'init': 'epsg:3006'})\n",
        "    lslide_shift = lslide_point.apply(lambda x: shapely.affinity.translate(x, xoff=x_shift, yoff=y_shift))\n",
        "\n",
        "    # Get a square buffer in METERS of the buffer\n",
        "    # lslide_20 = lslide_point.buffer(17.3970631342, resolution=1, cap_style=3)\n",
        "    \n",
        "    lslide_20 = lslide_shift.buffer(args.buffer_meters, resolution=1, cap_style = 3)\n",
        "\n",
        "    lslide_20 = lslide_20.to_crs(epsg=3857)\n",
        "\n",
        "    df = lslide_df['geometry'].within(lslide_20)\n",
        "\n",
        "    if (df.sum() >= 1):\n",
        "      # means the landslide geo dataframe contained the random point\n",
        "      continue\n",
        "    else:\n",
        "      ret = np.zeros((len(layer_types), 24, 24))\n",
        "      layer_count = 0\n",
        "\n",
        "      # extract the xarray values for each layer type, appending nan's as neccesary \n",
        "      # to ensure each layer is of size 24 x 24\n",
        "      for layer in layer_types:\n",
        "        next_layer = py3dep.get_map(layer, lslide_20.geometry[0], resolution=1, geo_crs=\"epsg:3857\", crs=\"epsg:3857\")\n",
        "        next_layer = next_layer.values\n",
        "        if next_layer.shape[0] != 24:\n",
        "          nan_row = np.empty([1, next_layer.shape[1]])\n",
        "          next_layer = np.concatenate((next_layer, nan_row), axis = 0)\n",
        "        if next_layer.shape[1] != 24:\n",
        "          nan_col = np.empty([24, 1])\n",
        "          next_layer = np.concatenate((next_layer, nan_col), axis = 1)\n",
        "\n",
        "        # Change nan's to 0's \n",
        "        # TO_CHANGE?\n",
        "        next_layer = np.nan_to_num(next_layer)\n",
        "\n",
        "        ret[layer_count] = next_layer \n",
        "        layer_count += 1\n",
        "\n",
        "      if debugging:\n",
        "        fig, ax = plt.subplots(figsize=(8,6))\n",
        "        lslide_point = lslide_point.to_crs(epsg=3857)\n",
        "        dem20.plot(ax=ax)\n",
        "        lslide_point.plot(ax=ax, color=\"red\")\n",
        "        print(dem20.shape)\n",
        "        print(np.unique(dem20.data))\n",
        "\n",
        "      return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pOTlnYhPaZQ"
      },
      "source": [
        "def get_landslide(row, layer_types = [\"DEM\"], debugging=False, radius_shift=8):\n",
        "  \"\"\"\n",
        "  Options for layer_types = OPTIONAL (Can select multiple)\n",
        "    \"DEM\"\n",
        "    \"Hillshade Gray\"\n",
        "    \"Aspect Degrees\"\n",
        "    \"Aspect Map\"\n",
        "    \"GreyHillshade_elevationFill\"\n",
        "    \"Hillshade Multidirectional\"\n",
        "    \"Slope Map\"\n",
        "    \"Slope Degrees\"\n",
        "    \"Hillshade Elevation Tinted\"\n",
        "    \"Height Ellipsoidal\"\n",
        "    \"Contour 25\"\n",
        "    \"Contour Smoothed 25\"\n",
        "  \"\"\"\n",
        "\n",
        "  lslide_point = gpd.GeoSeries([row.geometry])\n",
        "  lslide_point = lslide_point.set_crs(epsg=3857)\n",
        "  lslide_point = lslide_point.to_crs({'init': 'epsg:3006'})\n",
        "\n",
        "  x_shift = random.randint(radius_shift*-1, radius_shift)\n",
        "  y_shift = random.randint(radius_shift*-1, radius_shift)\n",
        "\n",
        "  lslide_shift = lslide_point.apply(lambda x: shapely.affinity.translate(x, xoff=x_shift, yoff=y_shift))\n",
        "\n",
        "  lslide_20 = lslide_shift.buffer(args.buffer_meters, resolution=1, cap_style = 3) \n",
        "\n",
        "  lslide_20 = lslide_20.to_crs(epsg=3857)\n",
        "\n",
        "  ret = np.zeros((len(layer_types), 24, 24))\n",
        "  layer_count = 0\n",
        "\n",
        "  for layer_type in layer_types:\n",
        "    next_layer = py3dep.get_map(layer_type, lslide_20.geometry[0], resolution=1, geo_crs=\"epsg:3857\", crs=\"epsg:3857\")\n",
        "    \n",
        "    # Change nan's to 0's\n",
        "    # TO_CHANGE?\n",
        "    next_layer_vals = np.nan_to_num(next_layer.values)\n",
        "\n",
        "    ret[layer_count] = next_layer_vals \n",
        "    layer_count += 1\n",
        "\n",
        "  if debugging:\n",
        "    fig, ax = plt.subplots(figsize=(8,6))\n",
        "    lslide_point = lslide_point.to_crs(epsg=3857)\n",
        "    dem20.plot(ax=ax)\n",
        "    lslide_point.plot(ax=ax, color=\"red\")\n",
        "    print(dem20.shape)\n",
        "\n",
        "  return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5WzE-eVv9KI"
      },
      "source": [
        "class DEM_Dataset(Dataset):\n",
        "    def __init__(self, num_samples, layers = [\"DEM\"], data_transforms=None, saved_data = True, train = True, val = False):\n",
        "      #if data_transforms is None: \n",
        "      #  self.transform = torchvision.transforms.Compose([                                          \n",
        "      #      torchvision.transforms.ToTensor()                                             \n",
        "      #  ])\n",
        "      #else: \n",
        "      #  self.transform = data_transforms\n",
        "\n",
        "      # Whether or not using saved data\n",
        "      if saved_data:\n",
        "        # Training dataset will have 80 samples of both pos and neg\n",
        "        # Validating dataset will have 20 each\n",
        "        if train:\n",
        "          all_landslides = np.zeros((1200, 4, 24, 24))\n",
        "\n",
        "          for i in range(1,601):\n",
        "            fnam = \"drive/MyDrive/Landslide Project/Positive_Samples/pos_sample\" + str(i)\n",
        "            with open(fnam, 'rb') as f:\n",
        "              next_sample = np.load(f,allow_pickle= True)\n",
        "              #next_sample = np.rollaxis(np.load(f,allow_pickle= True), 1)\n",
        "            all_landslides[i - 1] = next_sample\n",
        "\n",
        "          for i in range(1,601):\n",
        "            fnam = \"drive/MyDrive/Landslide Project/Negative_Samples/neg_sample\" + str(i)\n",
        "            with open(fnam, 'rb') as f:\n",
        "              next_sample = np.load(f,allow_pickle= True)\n",
        "              #next_sample = np.rollaxis(np.load(f, allow_pickle= True), 1)\n",
        "            all_landslides[600 + i - 1] = next_sample\n",
        "\n",
        "          print(all_landslides[0].shape)\n",
        "\n",
        "          y = [1 for i in range(600)] + [0 for i in range(600)]\n",
        "        \n",
        "        elif val: \n",
        "          all_landslides = np.zeros((200, 4, 24, 24))\n",
        "\n",
        "          for i in range(601, 701):\n",
        "            fnam = \"drive/MyDrive/Landslide Project/Positive_Samples/pos_sample\" + str(i)\n",
        "            with open(fnam, 'rb') as f:\n",
        "              next_sample = np.load(f,allow_pickle= True)\n",
        "              #next_sample = np.rollaxis(np.load(f, allow_pickle= True), 1)\n",
        "            all_landslides[i - 601] = next_sample\n",
        "\n",
        "          for i in range(601, 701):\n",
        "            fnam = \"drive/MyDrive/Landslide Project/Negative_Samples/neg_sample\" + str(i)\n",
        "            with open(fnam, 'rb') as f:\n",
        "              next_sample = np.load(f,allow_pickle= True)\n",
        "              #next_sample = np.rollaxis(np.load(f, allow_pickle= True), 1)\n",
        "            all_landslides[i - 501] = next_sample\n",
        "\n",
        "          y = [1 for i in range(100)] + [0 for i in range(100)]\n",
        "\n",
        "        else:\n",
        "          all_landslides = np.zeros((200, 4, 24, 24))\n",
        "\n",
        "          for i in range(701, 801):\n",
        "            fnam = \"drive/MyDrive/Landslide Project/Positive_Samples/pos_sample\" + str(i)\n",
        "            with open(fnam, 'rb') as f:\n",
        "              next_sample = np.load(f,allow_pickle= True)\n",
        "              #next_sample = np.rollaxis(np.load(f, allow_pickle= True), 1)\n",
        "            all_landslides[i - 701] = next_sample\n",
        "\n",
        "          for i in range(701, 801):\n",
        "            fnam = \"drive/MyDrive/Landslide Project/Negative_Samples/neg_sample\" + str(i)\n",
        "            with open(fnam, 'rb') as f:\n",
        "              next_sample = np.load(f,allow_pickle= True)\n",
        "              #next_sample = np.rollaxis(np.load(f, allow_pickle= True), 1)\n",
        "            all_landslides[i - 601] = next_sample\n",
        "\n",
        "          y = [1 for i in range(100)] + [0 for i in range(100)]\n",
        "\n",
        "      else:\n",
        "        # Initialize Numpy Array with 4 dimensions: (samples, layers, (24x24 geographical region))\n",
        "        all_landslides = np.zeros((num_samples * 2, len(layers), 24, 24))\n",
        "        sample_counter = 0\n",
        "\n",
        "        # Collect positive landslide samples\n",
        "        for index,row in landslide_filtered.sample(frac=1).iterrows():\n",
        "          dem_map = get_landslide(row, layer_types = layers)\n",
        "          all_landslides[sample_counter] = dem_map\n",
        "          print(\"Positive sample retrieved\")\n",
        "          sample_counter += 1\n",
        "          if sample_counter == num_samples:\n",
        "            break\n",
        "\n",
        "        # Collect negative landslide samples\n",
        "        for i in range(num_samples):\n",
        "          dem_map = get_no_landslide(landslide_gdf, layer_types = layers)\n",
        "          all_landslides[sample_counter] = dem_map\n",
        "          print(\"Negative sample retrieved\")\n",
        "          sample_counter += 1\n",
        "\n",
        "        y = [1 for i in range(num_samples)] + [0 for i in range(num_samples)]\n",
        "\n",
        "      self.dems = all_landslides\n",
        "      # 1 is positive sample, 0 is negative sample\n",
        "      self.labels = y\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        return a tuple: img, label\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "          idx = idx.tolist()\n",
        "        \n",
        "        #dem = self.transform(self.dems[idx])\n",
        "        dem_np = self.dems[idx]\n",
        "\n",
        "        # Normalize each layer\n",
        "        for i in range(0, self.dems.shape[1]):\n",
        "          cur_layer = dem_np[i]\n",
        "          if cur_layer[cur_layer != 0].size != 0:\n",
        "            mean = np.mean(cur_layer[cur_layer != 0])\n",
        "            std = np.std(cur_layer[cur_layer != 0])\n",
        "            cur_layer[cur_layer != 0] -= mean\n",
        "            cur_layer = cur_layer / std\n",
        "            dem_np[i] = cur_layer  \n",
        "\n",
        "        dem = torch.from_numpy(dem_np)\n",
        "        label = self.labels[idx]\n",
        "        sample = (dem,label)\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3475sxnj5Cid",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "7e1f9d2c-77be-44a9-b453-b99b99e62faa"
      },
      "source": [
        "def test_dataset(num_samples, saved_data, train):\n",
        "  train_dataset = DEM_Dataset(num_samples, layers = [\"DEM\", \"Slope Degrees\", \"Aspect Degrees\", \"Contour 25\"], saved_data = saved_data, train = train)\n",
        "\n",
        "  # Tests for __len__\n",
        "  assert type(len(train_dataset)) == int, f\"The output type for __len__ should be an int. Yours is {type(len(train_dataset))}\"\n",
        "\n",
        "  assert len(train_dataset) == 160, f\"Incorrect number of training examples returned. It should be 21000. Yours is {len(train_dataset)}\"\n",
        "\n",
        "  # Tests for __getitem__\n",
        "  assert type(train_dataset[0]) == tuple, \"Incorrect output type for __getitem_, tuples should be returned\"\n",
        "  assert len(train_dataset[0]) == 2, \"Incorrect output for __getitem__, a tuple containing two elements should be returned\"\n",
        "\n",
        "  assert train_dataset[0][0].shape == torch.Size([4, 24, 24]), \"Incorrect output for __getitem__, a tuple containing two elements: (1) a tensor of shape (3,64,64) and (2) a label (int) should be returned\"\n",
        "  assert train_dataset[0][1] == 0 or train_dataset[0][1] == 1, \"Boink\"\n",
        "\n",
        "  print(\"[SUCCESSFUL!] Congrats! Your implementation of dataload looks alright so far! You'll run visualize_data() function below to see if it can return data sample correct!\")\n",
        "\n",
        "test_dataset(1, True, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7752e9a8dcc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[SUCCESSFUL!] Congrats! Your implementation of dataload looks alright so far! You'll run visualize_data() function below to see if it can return data sample correct!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-7752e9a8dcc3>\u001b[0m in \u001b[0;36mtest_dataset\u001b[0;34m(num_samples, saved_data, train)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEM_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"DEM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Slope Degrees\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Aspect Degrees\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Contour 25\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Tests for __len__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The output type for __len__ should be an int. Yours is {type(len(train_dataset))}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-741d0d2ef71a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_samples, layers, data_transforms, saved_data, train, val)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mfnam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"drive/MyDrive/Landslide Project/Positive_Samples/pos_sample\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m               \u001b[0mnext_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m               \u001b[0;31m#next_sample = np.rollaxis(np.load(f,allow_pickle= True), 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mall_landslides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70PZj_hJqHnn"
      },
      "source": [
        "# script to save data to drive\n",
        "data = DEM_Dataset(args.num_samples, layers = args.layer_types, saved_data = False, train = False)\n",
        "\n",
        "for i in range(0, args.num_samples):\n",
        "  fnam = 'drive/MyDrive/Positive_Samples/pos_sample' + str(i + 1)\n",
        "  print(fnam)\n",
        "  with open(fnam, 'wb') as f:\n",
        "      np.save(f, data[i][0])\n",
        "\n",
        "for i in range(0, args.num_samples):\n",
        "  fnam = 'drive/MyDrive/Negative_Samples/neg_sample' + str(i + 1)\n",
        "  print(fnam)\n",
        "  with open(fnam, 'wb') as f:\n",
        "      np.save(f, data[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp9ej5lfkCKK"
      },
      "source": [
        "# script to save data to drive without creating dataset\n",
        "num_to_get = 1000\n",
        "sample_counter = 0\n",
        "\n",
        "# Save Positive Samples\n",
        "for index,row in landslide_filtered.sample(frac=1).iterrows():\n",
        "  dem_map = get_landslide(row, layer_types = args.layer_types)\n",
        "  fnam = 'drive/MyDrive/Landslide Project/Positive_Samples/pos_sample' + str(sample_counter + 1)\n",
        "  with open(fnam, 'wb') as f:\n",
        "      np.save(f, dem_map)\n",
        "  print(\"Positive sample retrieved: Number: \" + str(sample_counter + 1))\n",
        "  sample_counter += 1\n",
        "  if sample_counter == num_to_get:\n",
        "    break\n",
        "\n",
        "# Save Negative Samples\n",
        "for i in range(num_to_get):\n",
        "  dem_map = get_no_landslide(landslide_gdf, layer_types = args.layer_types)\n",
        "  fnam = 'drive/MyDrive/Landslide Project/Negative_Samples/neg_sample' + str(i + 1)\n",
        "  with open(fnam, 'wb') as f:\n",
        "      np.save(f, dem_map)\n",
        "  print(\"Negative sample retrieved: Number: \" + str(i + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ZJHrC_DFJK"
      },
      "source": [
        "# Set up CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYnM4spNLFas"
      },
      "source": [
        "def init_weights(net):\n",
        "    \"\"\"\n",
        "    Usage: net = Model()\n",
        "           net.apply(init_weights)\n",
        "    \"\"\"\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "#             if m.bias is not None:\n",
        "#                 stdv = 1. / math.sqrt(m.weight.size(1))\n",
        "#                 nn.init.uniform_(m.bias, -stdv, stdv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWkOhfLyRIWx"
      },
      "source": [
        "class CNN_Topo(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Task: create a model with:\n",
        "          2 convolutional layers, followed by 2 linear layers, the last of which should output the logits for each class.\n",
        "          Check the table given above (section 3.3.2) for more details in the specification.\n",
        "        \"\"\"\n",
        "        # Don't remove the following line. Otherwise, it would raise ```AttributeError: cannot assign module before Module.__init__() call``` exception ERROR!\n",
        "        super(CNN_Topo, self).__init__() \n",
        "        \n",
        "        # YOUR CODE HERE \n",
        "        self.conv1 = nn.Conv2d(4, 64, 3, 1, 1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)\n",
        "        self.pool3 = nn.MaxPool2d(2, 1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 3, 1, 1)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(51200, 500)\n",
        "        self.fc2 = nn.Linear(500, 50)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Your code here\n",
        "        @Brief: This function takes as input a tensor x of size Bx3x64x64 \n",
        "        and outputs a \"logit\" tensor of size Bx6. Do not include a softmax layer \n",
        "        here because most of Pytorch's loss functions take \"logit\" as one of the inputs\n",
        "        while integrating log() with softmax() into a log_softmax() function.    \n",
        "        @Inputs: \n",
        "          x: torch.Tensor((B,3,64,64)) \n",
        "        @return: torch.Tensor((B,6))\n",
        "        @Note: After the 2nd Conv2d (and ReLU), the intermediate feature has the shape \n",
        "               ```B x 16 x 14 x 14```. Before putting it into layer 3 (Linear), \n",
        "               make sure to reshape this intermediate feature to ```B x 3136```.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE \n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool4(x)\n",
        "        \n",
        "\n",
        "        # unclear what activation function is between linear layers\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5JJgVrK1Cqs"
      },
      "source": [
        "def save_model(model, name):\n",
        "    if isinstance(model, CNN_Topo):\n",
        "        return save(model.state_dict(), name + \".pth\")\n",
        "    \n",
        "    raise ValueError(\"model type '%s' not supported!\"%str(type(model)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q29lyEZCW8uK"
      },
      "source": [
        "class Args(object):\n",
        "  def __init__(self):\n",
        "    self.log_dir = './my_tensorboard_log_directory'\n",
        "    self.learning_rate = 0.001\n",
        "    self.momentum = 0.9\n",
        "    self.epochs = 15\n",
        "    self.model_name = \"drive/MyDrive/Landslide Project/Landslide Prediction CNN\"\n",
        "    self.batch_size = 10\n",
        "    self.device = \"cpu\"\n",
        "    self.num_samples = 450\n",
        "    self.buffer_meters = 10\n",
        "    self.length_min = 10\n",
        "    self.width_min = 10\n",
        "    self.dataset_size = 5\n",
        "    self.layer_types = [\"DEM\", \"Slope Degrees\", \"Aspect Degrees\", \"Contour 25\"]\n",
        "\n",
        "args = Args();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6TAeHCp1JN"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.log_softmax(y_pred, dim = 1)\n",
        "    _, y_pred_tags = torch.max(y_pred_tag, dim = 1)\n",
        "    correct_results_sum = (y_pred_tags == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-p_16EJXnX0"
      },
      "source": [
        "def train(args, model):\n",
        "    \"\"\"\n",
        "    @Brief: training your model. This should include the following items:\n",
        "        - Initialize the model (already given). Only need to map the model to the device on which you would want to run the model on \n",
        "                using the following syntax: \n",
        "                model = model.to(device) \n",
        "                where device = torch.device(<device_name>), \n",
        "                i.e: device = torch.device(\"cuda:0\") or device = torech.device(\"cpu\")\n",
        "                    \n",
        "        - Initialize tensorboard summarizers (already given)\n",
        "        - Initialize data loaders (you need to code up)\n",
        "        - Initialize the optimizer (you need to code up. Type is of your choice)\n",
        "        - Initialize the loss function (you should have coded up above)\n",
        "        - A for loop to iterate through many epochs (up to your choice). In each epoch:\n",
        "                - Iterate through every mini-batches (remember to map data and labels to the device that you would want to run the model on)\n",
        "                        - Run the forward path\n",
        "                        - Get loss\n",
        "                        - Calculate gradients \n",
        "                        - Update the model's parameters\n",
        "                - Evaluate your model on the validation set\n",
        "                - Save the model if the performance on the validation set is better using exactly the following line:\n",
        "                        save_model(model, model_name) \n",
        "                 \n",
        "    @Inputs: \n",
        "        Args: object of your choice to carry arguments that you want to use within your training function. \n",
        "    @Output: \n",
        "        No return is necessary here. \n",
        "    \"\"\"\n",
        "    # Initialize tensorboard loggers\n",
        "    if args.log_dir is not None:\n",
        "        train_logger = tb.SummaryWriter(path.join(args.log_dir, 'train'))\n",
        "        valid_logger = tb.SummaryWriter(path.join(args.log_dir, 'valid'))\n",
        "    \n",
        "    # Create subfolders to save the tensorboard log files\n",
        "    model_name = args.model_name\n",
        "    if not os.path.exists(path.join(args.log_dir, f'train/{model_name}')):\n",
        "        os.makedirs(path.join(args.log_dir, f'train/{model_name}'))\n",
        "    if not os.path.exists(path.join(args.log_dir, f'valid/{model_name}')):\n",
        "        os.makedirs(path.join(args.log_dir, f'valid/{model_name}'))  \n",
        "    #----------------------------------------\n",
        "\n",
        "    # Write model to device\n",
        "    device = torch.device(args.device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create training and Evaluating Datasets\n",
        "    train_data = DEM_Dataset(args.num_samples, layers = args.layer_types)\n",
        "    print(\"Training Data Complete\")\n",
        "    eval_data = DEM_Dataset(args.num_samples, layers = args.layer_types, train = False, val = True)\n",
        "    print(\"Evaluation Model Complete\")\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size,\n",
        "                                          shuffle=True)\n",
        "    valloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "    # Definte Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum)\n",
        "\n",
        "    # Initialize performance statistics dictionaries\n",
        "    accuracy_stats = {\n",
        "        'train': [],\n",
        "        \"val\": []\n",
        "    }\n",
        "    loss_stats = {\n",
        "        'train': [],\n",
        "        \"val\": []\n",
        "    }\n",
        "    \n",
        "    # TRAINING\n",
        "\n",
        "    for epoch in range(args.epochs):  # loop over the dataset multiple times\n",
        "      print(\"Starting Next Epoch\")\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_acc = 0.0\n",
        "      print(\"Begin Training for Current Epoch\")\n",
        "      model.train()\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          input, label = data\n",
        "          # nan_to_num in dataset creation but nan's still appearing in inputs - not sure why\n",
        "          input[torch.isnan(input)] = 0\n",
        "          input, label = input.to(device), label.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          output = model(input.double()) \n",
        "\n",
        "          train_loss = criterion(output, label)\n",
        "          train_loss.backward()\n",
        "          \n",
        "          # Calculate accuracy\n",
        "          train_acc = binary_acc(output, label)\n",
        "          optimizer.step()\n",
        "\n",
        "          # print and save statistics\n",
        "          running_loss += train_loss.item()\n",
        "          running_acc += train_acc.item()\n",
        "          if i % 5 == 4:    # print every 5 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / i))\n",
        "            print('[%d, %5d] acc: %.3f'% (epoch + 1, i + 1, running_acc / i))\n",
        "\n",
        "      # VALIDATION\n",
        "      with torch.no_grad():\n",
        "          print(\"Begin Evaluation for Current Epoch\")\n",
        "          model.eval()\n",
        "          running_val_loss = 0.0\n",
        "          running_val_acc = 0.0\n",
        "\n",
        "          for i, val_data in enumerate(valloader,0):\n",
        "              val_input, val_label = val_data\n",
        "              # nan_to_num in dataset creation but nan's still appearing in inputs - not sure why\n",
        "              val_input[torch.isnan(val_input)] = 0\n",
        "              val_input, val_label = val_input.to(device), val_label.to(device)\n",
        "              \n",
        "              output = model(val_input)\n",
        "\n",
        "              # calculate and save statistics\n",
        "              val_loss = criterion(output, val_label)\n",
        "              val_acc = binary_acc(output, val_label)\n",
        "\n",
        "              running_val_loss += val_loss.item()\n",
        "              running_val_acc += val_acc.item()\n",
        "\n",
        "              if i % 5 == 4:    # print every 5 mini-batches\n",
        "                print('[%d, %5d] val loss: %.3f' % (epoch + 1, i + 1, running_val_loss / (i+1)))\n",
        "                print('[%d, %5d] val acc: %.3f'% (epoch + 1, i + 1, running_val_acc / (i+1)))\n",
        "\n",
        "      loss_stats['train'].append(running_loss/len(trainloader))\n",
        "      loss_stats['val'].append(running_val_loss/len(valloader))\n",
        "      accuracy_stats['train'].append(running_acc/len(trainloader))\n",
        "      accuracy_stats['val'].append(running_val_acc/len(valloader))\n",
        "      print(f'Epoch {epoch+1:02}: | Train Loss: {running_loss/len(trainloader):.5f} | Val Loss: {running_val_loss/len(valloader):.5f} | Train Acc: {running_acc/len(trainloader):.3f}| Val Acc: {running_val_acc/len(valloader):.3f}')\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "    train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
        "    sns.lineplot(data=train_val_acc_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')\n",
        "    sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val Loss/Epoch')\n",
        "\n",
        "\n",
        "    # Don't touch the following lines\n",
        "    #You are not returning a model, but rather saving it to a file, which you will upload along with the homework4.py file\n",
        "    save_model(model, model_name) \n",
        "    # Make sure the file has been saved \n",
        "    assert os.path.exists(model_name + \".pth\") and os.path.isfile(model_name + \".pth\"), f\"[Fail to save your model named {model_name}.pth!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyH9D1_TDunv"
      },
      "source": [
        "# Train and Evaluate CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "20HRsyuJg3Os",
        "outputId": "891e9b26-36a6-4092-87e6-ff8b6a46f8bd"
      },
      "source": [
        "model = CNN_Topo()\n",
        "model.apply(init_weights)\n",
        "model = model.double()\n",
        "train(args, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 24, 24)\n",
            "Training Data Complete\n",
            "Evaluation Model Complete\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[1,     5] loss: 3.439\n",
            "[1,     5] acc: 72.500\n",
            "[1,    10] loss: 2.372\n",
            "[1,    10] acc: 60.000\n",
            "[1,    15] loss: 1.802\n",
            "[1,    15] acc: 62.857\n",
            "[1,    20] loss: 1.539\n",
            "[1,    20] acc: 66.316\n",
            "[1,    25] loss: 1.344\n",
            "[1,    25] acc: 65.833\n",
            "[1,    30] loss: 1.212\n",
            "[1,    30] acc: 66.552\n",
            "[1,    35] loss: 1.121\n",
            "[1,    35] acc: 65.882\n",
            "[1,    40] loss: 1.036\n",
            "[1,    40] acc: 67.179\n",
            "[1,    45] loss: 0.979\n",
            "[1,    45] acc: 67.727\n",
            "[1,    50] loss: 0.930\n",
            "[1,    50] acc: 68.163\n",
            "[1,    55] loss: 0.878\n",
            "[1,    55] acc: 69.815\n",
            "[1,    60] loss: 0.852\n",
            "[1,    60] acc: 70.000\n",
            "[1,    65] loss: 0.813\n",
            "[1,    65] acc: 71.094\n",
            "[1,    70] loss: 0.792\n",
            "[1,    70] acc: 71.159\n",
            "[1,    75] loss: 0.770\n",
            "[1,    75] acc: 71.757\n",
            "[1,    80] loss: 0.753\n",
            "[1,    80] acc: 71.392\n",
            "[1,    85] loss: 0.736\n",
            "[1,    85] acc: 71.667\n",
            "[1,    90] loss: 0.725\n",
            "[1,    90] acc: 71.461\n",
            "[1,    95] loss: 0.714\n",
            "[1,    95] acc: 71.170\n",
            "[1,   100] loss: 0.701\n",
            "[1,   100] acc: 71.616\n",
            "[1,   105] loss: 0.688\n",
            "[1,   105] acc: 71.923\n",
            "[1,   110] loss: 0.676\n",
            "[1,   110] acc: 72.202\n",
            "[1,   115] loss: 0.669\n",
            "[1,   115] acc: 72.193\n",
            "[1,   120] loss: 0.657\n",
            "[1,   120] acc: 72.605\n",
            "Begin Evaluation for Current Epoch\n",
            "[1,     5] val loss: 0.409\n",
            "[1,     5] val acc: 82.000\n",
            "[1,    10] val loss: 0.376\n",
            "[1,    10] val acc: 82.000\n",
            "[1,    15] val loss: 0.379\n",
            "[1,    15] val acc: 81.333\n",
            "[1,    20] val loss: 0.388\n",
            "[1,    20] val acc: 80.500\n",
            "[1,    25] val loss: 0.398\n",
            "[1,    25] val acc: 78.400\n",
            "[1,    30] val loss: 0.409\n",
            "[1,    30] val acc: 78.000\n",
            "[1,    35] val loss: 0.425\n",
            "[1,    35] val acc: 76.000\n",
            "[1,    40] val loss: 0.423\n",
            "[1,    40] val acc: 76.500\n",
            "[1,    45] val loss: 0.428\n",
            "[1,    45] val acc: 75.778\n",
            "[1,    50] val loss: 0.435\n",
            "[1,    50] val acc: 75.000\n",
            "[1,    55] val loss: 0.440\n",
            "[1,    55] val acc: 74.182\n",
            "[1,    60] val loss: 0.442\n",
            "[1,    60] val acc: 74.167\n",
            "[1,    65] val loss: 0.439\n",
            "[1,    65] val acc: 74.615\n",
            "[1,    70] val loss: 0.432\n",
            "[1,    70] val acc: 75.286\n",
            "[1,    75] val loss: 0.432\n",
            "[1,    75] val acc: 75.200\n",
            "[1,    80] val loss: 0.430\n",
            "[1,    80] val acc: 75.250\n",
            "[1,    85] val loss: 0.424\n",
            "[1,    85] val acc: 75.882\n",
            "[1,    90] val loss: 0.420\n",
            "[1,    90] val acc: 76.444\n",
            "[1,    95] val loss: 0.422\n",
            "[1,    95] val acc: 76.526\n",
            "[1,   100] val loss: 0.422\n",
            "[1,   100] val acc: 76.600\n",
            "[1,   105] val loss: 0.421\n",
            "[1,   105] val acc: 76.857\n",
            "[1,   110] val loss: 0.423\n",
            "[1,   110] val acc: 76.545\n",
            "[1,   115] val loss: 0.428\n",
            "[1,   115] val acc: 76.087\n",
            "[1,   120] val loss: 0.430\n",
            "[1,   120] val acc: 76.083\n",
            "Epoch 01: | Train Loss: 0.65170 | Val Loss: 0.43030 | Train Acc: 72.000| Val Acc: 76.083\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[2,     5] loss: 0.467\n",
            "[2,     5] acc: 100.000\n",
            "[2,    10] loss: 0.448\n",
            "[2,    10] acc: 88.889\n",
            "[2,    15] loss: 0.456\n",
            "[2,    15] acc: 81.429\n",
            "[2,    20] loss: 0.449\n",
            "[2,    20] acc: 81.053\n",
            "[2,    25] loss: 0.453\n",
            "[2,    25] acc: 81.250\n",
            "[2,    30] loss: 0.438\n",
            "[2,    30] acc: 82.414\n",
            "[2,    35] loss: 0.432\n",
            "[2,    35] acc: 81.176\n",
            "[2,    40] loss: 0.441\n",
            "[2,    40] acc: 80.000\n",
            "[2,    45] loss: 0.452\n",
            "[2,    45] acc: 78.182\n",
            "[2,    50] loss: 0.441\n",
            "[2,    50] acc: 79.388\n",
            "[2,    55] loss: 0.446\n",
            "[2,    55] acc: 79.444\n",
            "[2,    60] loss: 0.450\n",
            "[2,    60] acc: 78.983\n",
            "[2,    65] loss: 0.446\n",
            "[2,    65] acc: 79.375\n",
            "[2,    70] loss: 0.444\n",
            "[2,    70] acc: 79.420\n",
            "[2,    75] loss: 0.443\n",
            "[2,    75] acc: 79.865\n",
            "[2,    80] loss: 0.440\n",
            "[2,    80] acc: 80.380\n",
            "[2,    85] loss: 0.439\n",
            "[2,    85] acc: 80.119\n",
            "[2,    90] loss: 0.430\n",
            "[2,    90] acc: 80.674\n",
            "[2,    95] loss: 0.430\n",
            "[2,    95] acc: 80.426\n",
            "[2,   100] loss: 0.430\n",
            "[2,   100] acc: 80.505\n",
            "[2,   105] loss: 0.427\n",
            "[2,   105] acc: 80.769\n",
            "[2,   110] loss: 0.430\n",
            "[2,   110] acc: 80.550\n",
            "[2,   115] loss: 0.428\n",
            "[2,   115] acc: 80.614\n",
            "[2,   120] loss: 0.428\n",
            "[2,   120] acc: 80.420\n",
            "Begin Evaluation for Current Epoch\n",
            "[2,     5] val loss: 0.570\n",
            "[2,     5] val acc: 68.000\n",
            "[2,    10] val loss: 0.440\n",
            "[2,    10] val acc: 77.000\n",
            "[2,    15] val loss: 0.385\n",
            "[2,    15] val acc: 81.333\n",
            "[2,    20] val loss: 0.346\n",
            "[2,    20] val acc: 82.500\n",
            "[2,    25] val loss: 0.341\n",
            "[2,    25] val acc: 82.800\n",
            "[2,    30] val loss: 0.359\n",
            "[2,    30] val acc: 81.333\n",
            "[2,    35] val loss: 0.355\n",
            "[2,    35] val acc: 82.000\n",
            "[2,    40] val loss: 0.363\n",
            "[2,    40] val acc: 81.250\n",
            "[2,    45] val loss: 0.357\n",
            "[2,    45] val acc: 81.111\n",
            "[2,    50] val loss: 0.362\n",
            "[2,    50] val acc: 80.800\n",
            "[2,    55] val loss: 0.354\n",
            "[2,    55] val acc: 81.455\n",
            "[2,    60] val loss: 0.353\n",
            "[2,    60] val acc: 81.500\n",
            "[2,    65] val loss: 0.359\n",
            "[2,    65] val acc: 81.231\n",
            "[2,    70] val loss: 0.371\n",
            "[2,    70] val acc: 80.429\n",
            "[2,    75] val loss: 0.367\n",
            "[2,    75] val acc: 80.800\n",
            "[2,    80] val loss: 0.358\n",
            "[2,    80] val acc: 81.125\n",
            "[2,    85] val loss: 0.353\n",
            "[2,    85] val acc: 81.294\n",
            "[2,    90] val loss: 0.353\n",
            "[2,    90] val acc: 81.333\n",
            "[2,    95] val loss: 0.353\n",
            "[2,    95] val acc: 81.474\n",
            "[2,   100] val loss: 0.344\n",
            "[2,   100] val acc: 82.100\n",
            "[2,   105] val loss: 0.348\n",
            "[2,   105] val acc: 81.714\n",
            "[2,   110] val loss: 0.349\n",
            "[2,   110] val acc: 81.636\n",
            "[2,   115] val loss: 0.352\n",
            "[2,   115] val acc: 81.652\n",
            "[2,   120] val loss: 0.360\n",
            "[2,   120] val acc: 81.000\n",
            "Epoch 02: | Train Loss: 0.42480 | Val Loss: 0.36007 | Train Acc: 79.750| Val Acc: 81.000\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[3,     5] loss: 0.470\n",
            "[3,     5] acc: 105.000\n",
            "[3,    10] loss: 0.410\n",
            "[3,    10] acc: 94.444\n",
            "[3,    15] loss: 0.374\n",
            "[3,    15] acc: 92.143\n",
            "[3,    20] loss: 0.371\n",
            "[3,    20] acc: 89.474\n",
            "[3,    25] loss: 0.370\n",
            "[3,    25] acc: 88.750\n",
            "[3,    30] loss: 0.361\n",
            "[3,    30] acc: 88.276\n",
            "[3,    35] loss: 0.349\n",
            "[3,    35] acc: 88.529\n",
            "[3,    40] loss: 0.349\n",
            "[3,    40] acc: 87.949\n",
            "[3,    45] loss: 0.358\n",
            "[3,    45] acc: 86.591\n",
            "[3,    50] loss: 0.350\n",
            "[3,    50] acc: 87.143\n",
            "[3,    55] loss: 0.349\n",
            "[3,    55] acc: 86.667\n",
            "[3,    60] loss: 0.355\n",
            "[3,    60] acc: 85.763\n",
            "[3,    65] loss: 0.368\n",
            "[3,    65] acc: 85.156\n",
            "[3,    70] loss: 0.367\n",
            "[3,    70] acc: 84.783\n",
            "[3,    75] loss: 0.364\n",
            "[3,    75] acc: 85.405\n",
            "[3,    80] loss: 0.359\n",
            "[3,    80] acc: 85.570\n",
            "[3,    85] loss: 0.358\n",
            "[3,    85] acc: 85.238\n",
            "[3,    90] loss: 0.359\n",
            "[3,    90] acc: 85.169\n",
            "[3,    95] loss: 0.363\n",
            "[3,    95] acc: 84.787\n",
            "[3,   100] loss: 0.367\n",
            "[3,   100] acc: 84.646\n",
            "[3,   105] loss: 0.367\n",
            "[3,   105] acc: 84.615\n",
            "[3,   110] loss: 0.366\n",
            "[3,   110] acc: 84.587\n",
            "[3,   115] loss: 0.361\n",
            "[3,   115] acc: 85.175\n",
            "[3,   120] loss: 0.359\n",
            "[3,   120] acc: 85.462\n",
            "Begin Evaluation for Current Epoch\n",
            "[3,     5] val loss: 0.200\n",
            "[3,     5] val acc: 94.000\n",
            "[3,    10] val loss: 0.240\n",
            "[3,    10] val acc: 91.000\n",
            "[3,    15] val loss: 0.254\n",
            "[3,    15] val acc: 90.667\n",
            "[3,    20] val loss: 0.245\n",
            "[3,    20] val acc: 91.000\n",
            "[3,    25] val loss: 0.252\n",
            "[3,    25] val acc: 90.400\n",
            "[3,    30] val loss: 0.254\n",
            "[3,    30] val acc: 89.667\n",
            "[3,    35] val loss: 0.249\n",
            "[3,    35] val acc: 90.000\n",
            "[3,    40] val loss: 0.248\n",
            "[3,    40] val acc: 90.500\n",
            "[3,    45] val loss: 0.253\n",
            "[3,    45] val acc: 90.222\n",
            "[3,    50] val loss: 0.254\n",
            "[3,    50] val acc: 90.400\n",
            "[3,    55] val loss: 0.255\n",
            "[3,    55] val acc: 90.727\n",
            "[3,    60] val loss: 0.253\n",
            "[3,    60] val acc: 91.333\n",
            "[3,    65] val loss: 0.252\n",
            "[3,    65] val acc: 91.538\n",
            "[3,    70] val loss: 0.250\n",
            "[3,    70] val acc: 91.857\n",
            "[3,    75] val loss: 0.251\n",
            "[3,    75] val acc: 91.600\n",
            "[3,    80] val loss: 0.254\n",
            "[3,    80] val acc: 91.375\n",
            "[3,    85] val loss: 0.254\n",
            "[3,    85] val acc: 91.529\n",
            "[3,    90] val loss: 0.253\n",
            "[3,    90] val acc: 91.778\n",
            "[3,    95] val loss: 0.254\n",
            "[3,    95] val acc: 91.789\n",
            "[3,   100] val loss: 0.256\n",
            "[3,   100] val acc: 91.500\n",
            "[3,   105] val loss: 0.254\n",
            "[3,   105] val acc: 91.619\n",
            "[3,   110] val loss: 0.258\n",
            "[3,   110] val acc: 91.273\n",
            "[3,   115] val loss: 0.256\n",
            "[3,   115] val acc: 91.565\n",
            "[3,   120] val loss: 0.254\n",
            "[3,   120] val acc: 91.667\n",
            "Epoch 03: | Train Loss: 0.35583 | Val Loss: 0.25412 | Train Acc: 84.750| Val Acc: 91.667\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[4,     5] loss: 0.344\n",
            "[4,     5] acc: 107.500\n",
            "[4,    10] loss: 0.283\n",
            "[4,    10] acc: 100.000\n",
            "[4,    15] loss: 0.281\n",
            "[4,    15] acc: 95.714\n",
            "[4,    20] loss: 0.269\n",
            "[4,    20] acc: 94.737\n",
            "[4,    25] loss: 0.245\n",
            "[4,    25] acc: 94.167\n",
            "[4,    30] loss: 0.252\n",
            "[4,    30] acc: 93.448\n",
            "[4,    35] loss: 0.275\n",
            "[4,    35] acc: 91.765\n",
            "[4,    40] loss: 0.286\n",
            "[4,    40] acc: 90.769\n",
            "[4,    45] loss: 0.281\n",
            "[4,    45] acc: 90.000\n",
            "[4,    50] loss: 0.285\n",
            "[4,    50] acc: 88.980\n",
            "[4,    55] loss: 0.286\n",
            "[4,    55] acc: 88.148\n",
            "[4,    60] loss: 0.278\n",
            "[4,    60] acc: 88.305\n",
            "[4,    65] loss: 0.282\n",
            "[4,    65] acc: 88.125\n",
            "[4,    70] loss: 0.288\n",
            "[4,    70] acc: 87.536\n",
            "[4,    75] loss: 0.290\n",
            "[4,    75] acc: 86.892\n",
            "[4,    80] loss: 0.290\n",
            "[4,    80] acc: 87.215\n",
            "[4,    85] loss: 0.289\n",
            "[4,    85] acc: 87.381\n",
            "[4,    90] loss: 0.292\n",
            "[4,    90] acc: 86.966\n",
            "[4,    95] loss: 0.291\n",
            "[4,    95] acc: 87.021\n",
            "[4,   100] loss: 0.294\n",
            "[4,   100] acc: 86.869\n",
            "[4,   105] loss: 0.300\n",
            "[4,   105] acc: 86.635\n",
            "[4,   110] loss: 0.295\n",
            "[4,   110] acc: 86.972\n",
            "[4,   115] loss: 0.295\n",
            "[4,   115] acc: 87.018\n",
            "[4,   120] loss: 0.294\n",
            "[4,   120] acc: 87.143\n",
            "Begin Evaluation for Current Epoch\n",
            "[4,     5] val loss: 0.154\n",
            "[4,     5] val acc: 96.000\n",
            "[4,    10] val loss: 0.155\n",
            "[4,    10] val acc: 95.000\n",
            "[4,    15] val loss: 0.164\n",
            "[4,    15] val acc: 95.333\n",
            "[4,    20] val loss: 0.177\n",
            "[4,    20] val acc: 94.500\n",
            "[4,    25] val loss: 0.172\n",
            "[4,    25] val acc: 95.200\n",
            "[4,    30] val loss: 0.184\n",
            "[4,    30] val acc: 94.667\n",
            "[4,    35] val loss: 0.186\n",
            "[4,    35] val acc: 94.857\n",
            "[4,    40] val loss: 0.190\n",
            "[4,    40] val acc: 94.750\n",
            "[4,    45] val loss: 0.201\n",
            "[4,    45] val acc: 93.778\n",
            "[4,    50] val loss: 0.204\n",
            "[4,    50] val acc: 93.800\n",
            "[4,    55] val loss: 0.204\n",
            "[4,    55] val acc: 94.182\n",
            "[4,    60] val loss: 0.205\n",
            "[4,    60] val acc: 93.833\n",
            "[4,    65] val loss: 0.203\n",
            "[4,    65] val acc: 93.692\n",
            "[4,    70] val loss: 0.200\n",
            "[4,    70] val acc: 94.000\n",
            "[4,    75] val loss: 0.202\n",
            "[4,    75] val acc: 93.867\n",
            "[4,    80] val loss: 0.198\n",
            "[4,    80] val acc: 94.000\n",
            "[4,    85] val loss: 0.200\n",
            "[4,    85] val acc: 93.882\n",
            "[4,    90] val loss: 0.203\n",
            "[4,    90] val acc: 93.667\n",
            "[4,    95] val loss: 0.202\n",
            "[4,    95] val acc: 93.789\n",
            "[4,   100] val loss: 0.202\n",
            "[4,   100] val acc: 93.700\n",
            "[4,   105] val loss: 0.204\n",
            "[4,   105] val acc: 93.524\n",
            "[4,   110] val loss: 0.206\n",
            "[4,   110] val acc: 93.364\n",
            "[4,   115] val loss: 0.205\n",
            "[4,   115] val acc: 93.391\n",
            "[4,   120] val loss: 0.205\n",
            "[4,   120] val acc: 93.500\n",
            "Epoch 04: | Train Loss: 0.29159 | Val Loss: 0.20480 | Train Acc: 86.417| Val Acc: 93.500\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[5,     5] loss: 0.206\n",
            "[5,     5] acc: 122.500\n",
            "[5,    10] loss: 0.187\n",
            "[5,    10] acc: 108.889\n",
            "[5,    15] loss: 0.201\n",
            "[5,    15] acc: 101.429\n",
            "[5,    20] loss: 0.194\n",
            "[5,    20] acc: 99.474\n",
            "[5,    25] loss: 0.189\n",
            "[5,    25] acc: 98.750\n",
            "[5,    30] loss: 0.176\n",
            "[5,    30] acc: 98.276\n",
            "[5,    35] loss: 0.170\n",
            "[5,    35] acc: 97.353\n",
            "[5,    40] loss: 0.175\n",
            "[5,    40] acc: 96.154\n",
            "[5,    45] loss: 0.176\n",
            "[5,    45] acc: 95.682\n",
            "[5,    50] loss: 0.185\n",
            "[5,    50] acc: 95.102\n",
            "[5,    55] loss: 0.181\n",
            "[5,    55] acc: 95.370\n",
            "[5,    60] loss: 0.179\n",
            "[5,    60] acc: 95.085\n",
            "[5,    65] loss: 0.176\n",
            "[5,    65] acc: 95.156\n",
            "[5,    70] loss: 0.185\n",
            "[5,    70] acc: 94.783\n",
            "[5,    75] loss: 0.184\n",
            "[5,    75] acc: 94.595\n",
            "[5,    80] loss: 0.189\n",
            "[5,    80] acc: 94.177\n",
            "[5,    85] loss: 0.190\n",
            "[5,    85] acc: 94.167\n",
            "[5,    90] loss: 0.191\n",
            "[5,    90] acc: 94.157\n",
            "[5,    95] loss: 0.190\n",
            "[5,    95] acc: 94.043\n",
            "[5,   100] loss: 0.195\n",
            "[5,   100] acc: 93.434\n",
            "[5,   105] loss: 0.197\n",
            "[5,   105] acc: 93.365\n",
            "[5,   110] loss: 0.201\n",
            "[5,   110] acc: 93.119\n",
            "[5,   115] loss: 0.211\n",
            "[5,   115] acc: 92.719\n",
            "[5,   120] loss: 0.216\n",
            "[5,   120] acc: 92.269\n",
            "Begin Evaluation for Current Epoch\n",
            "[5,     5] val loss: 0.204\n",
            "[5,     5] val acc: 96.000\n",
            "[5,    10] val loss: 0.151\n",
            "[5,    10] val acc: 98.000\n",
            "[5,    15] val loss: 0.155\n",
            "[5,    15] val acc: 96.667\n",
            "[5,    20] val loss: 0.142\n",
            "[5,    20] val acc: 97.500\n",
            "[5,    25] val loss: 0.138\n",
            "[5,    25] val acc: 97.200\n",
            "[5,    30] val loss: 0.143\n",
            "[5,    30] val acc: 97.000\n",
            "[5,    35] val loss: 0.140\n",
            "[5,    35] val acc: 96.857\n",
            "[5,    40] val loss: 0.137\n",
            "[5,    40] val acc: 97.250\n",
            "[5,    45] val loss: 0.138\n",
            "[5,    45] val acc: 97.333\n",
            "[5,    50] val loss: 0.137\n",
            "[5,    50] val acc: 97.400\n",
            "[5,    55] val loss: 0.138\n",
            "[5,    55] val acc: 97.455\n",
            "[5,    60] val loss: 0.135\n",
            "[5,    60] val acc: 97.667\n",
            "[5,    65] val loss: 0.135\n",
            "[5,    65] val acc: 97.692\n",
            "[5,    70] val loss: 0.137\n",
            "[5,    70] val acc: 97.429\n",
            "[5,    75] val loss: 0.134\n",
            "[5,    75] val acc: 97.600\n",
            "[5,    80] val loss: 0.134\n",
            "[5,    80] val acc: 97.625\n",
            "[5,    85] val loss: 0.133\n",
            "[5,    85] val acc: 97.529\n",
            "[5,    90] val loss: 0.132\n",
            "[5,    90] val acc: 97.444\n",
            "[5,    95] val loss: 0.132\n",
            "[5,    95] val acc: 97.368\n",
            "[5,   100] val loss: 0.133\n",
            "[5,   100] val acc: 97.200\n",
            "[5,   105] val loss: 0.133\n",
            "[5,   105] val acc: 97.333\n",
            "[5,   110] val loss: 0.132\n",
            "[5,   110] val acc: 97.273\n",
            "[5,   115] val loss: 0.132\n",
            "[5,   115] val acc: 97.217\n",
            "[5,   120] val loss: 0.132\n",
            "[5,   120] val acc: 97.250\n",
            "Epoch 05: | Train Loss: 0.21386 | Val Loss: 0.13248 | Train Acc: 91.500| Val Acc: 97.250\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[6,     5] loss: 0.167\n",
            "[6,     5] acc: 115.000\n",
            "[6,    10] loss: 0.332\n",
            "[6,    10] acc: 97.778\n",
            "[6,    15] loss: 0.318\n",
            "[6,    15] acc: 94.286\n",
            "[6,    20] loss: 0.304\n",
            "[6,    20] acc: 93.684\n",
            "[6,    25] loss: 0.280\n",
            "[6,    25] acc: 93.333\n",
            "[6,    30] loss: 0.262\n",
            "[6,    30] acc: 94.138\n",
            "[6,    35] loss: 0.246\n",
            "[6,    35] acc: 94.412\n",
            "[6,    40] loss: 0.234\n",
            "[6,    40] acc: 94.615\n",
            "[6,    45] loss: 0.237\n",
            "[6,    45] acc: 93.864\n",
            "[6,    50] loss: 0.232\n",
            "[6,    50] acc: 93.673\n",
            "[6,    55] loss: 0.229\n",
            "[6,    55] acc: 93.519\n",
            "[6,    60] loss: 0.225\n",
            "[6,    60] acc: 93.220\n",
            "[6,    65] loss: 0.220\n",
            "[6,    65] acc: 93.281\n",
            "[6,    70] loss: 0.211\n",
            "[6,    70] acc: 93.623\n",
            "[6,    75] loss: 0.206\n",
            "[6,    75] acc: 93.784\n",
            "[6,    80] loss: 0.205\n",
            "[6,    80] acc: 93.418\n",
            "[6,    85] loss: 0.197\n",
            "[6,    85] acc: 93.810\n",
            "[6,    90] loss: 0.193\n",
            "[6,    90] acc: 94.045\n",
            "[6,    95] loss: 0.187\n",
            "[6,    95] acc: 94.149\n",
            "[6,   100] loss: 0.183\n",
            "[6,   100] acc: 94.343\n",
            "[6,   105] loss: 0.182\n",
            "[6,   105] acc: 94.231\n",
            "[6,   110] loss: 0.183\n",
            "[6,   110] acc: 94.128\n",
            "[6,   115] loss: 0.181\n",
            "[6,   115] acc: 94.211\n",
            "[6,   120] loss: 0.178\n",
            "[6,   120] acc: 94.286\n",
            "Begin Evaluation for Current Epoch\n",
            "[6,     5] val loss: 0.100\n",
            "[6,     5] val acc: 96.000\n",
            "[6,    10] val loss: 0.076\n",
            "[6,    10] val acc: 98.000\n",
            "[6,    15] val loss: 0.073\n",
            "[6,    15] val acc: 98.667\n",
            "[6,    20] val loss: 0.068\n",
            "[6,    20] val acc: 99.000\n",
            "[6,    25] val loss: 0.078\n",
            "[6,    25] val acc: 98.400\n",
            "[6,    30] val loss: 0.075\n",
            "[6,    30] val acc: 98.333\n",
            "[6,    35] val loss: 0.079\n",
            "[6,    35] val acc: 97.714\n",
            "[6,    40] val loss: 0.078\n",
            "[6,    40] val acc: 98.000\n",
            "[6,    45] val loss: 0.079\n",
            "[6,    45] val acc: 98.000\n",
            "[6,    50] val loss: 0.083\n",
            "[6,    50] val acc: 97.800\n",
            "[6,    55] val loss: 0.082\n",
            "[6,    55] val acc: 98.000\n",
            "[6,    60] val loss: 0.083\n",
            "[6,    60] val acc: 98.000\n",
            "[6,    65] val loss: 0.089\n",
            "[6,    65] val acc: 97.692\n",
            "[6,    70] val loss: 0.097\n",
            "[6,    70] val acc: 97.143\n",
            "[6,    75] val loss: 0.098\n",
            "[6,    75] val acc: 97.067\n",
            "[6,    80] val loss: 0.096\n",
            "[6,    80] val acc: 97.125\n",
            "[6,    85] val loss: 0.094\n",
            "[6,    85] val acc: 97.294\n",
            "[6,    90] val loss: 0.094\n",
            "[6,    90] val acc: 97.333\n",
            "[6,    95] val loss: 0.094\n",
            "[6,    95] val acc: 97.263\n",
            "[6,   100] val loss: 0.094\n",
            "[6,   100] val acc: 97.200\n",
            "[6,   105] val loss: 0.096\n",
            "[6,   105] val acc: 97.048\n",
            "[6,   110] val loss: 0.095\n",
            "[6,   110] val acc: 97.091\n",
            "[6,   115] val loss: 0.093\n",
            "[6,   115] val acc: 97.217\n",
            "[6,   120] val loss: 0.093\n",
            "[6,   120] val acc: 97.250\n",
            "Epoch 06: | Train Loss: 0.17686 | Val Loss: 0.09277 | Train Acc: 93.500| Val Acc: 97.250\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[7,     5] loss: 0.104\n",
            "[7,     5] acc: 120.000\n",
            "[7,    10] loss: 0.122\n",
            "[7,    10] acc: 107.778\n",
            "[7,    15] loss: 0.102\n",
            "[7,    15] acc: 104.286\n",
            "[7,    20] loss: 0.133\n",
            "[7,    20] acc: 101.053\n",
            "[7,    25] loss: 0.134\n",
            "[7,    25] acc: 99.583\n",
            "[7,    30] loss: 0.127\n",
            "[7,    30] acc: 98.966\n",
            "[7,    35] loss: 0.142\n",
            "[7,    35] acc: 97.353\n",
            "[7,    40] loss: 0.152\n",
            "[7,    40] acc: 96.410\n",
            "[7,    45] loss: 0.162\n",
            "[7,    45] acc: 95.909\n",
            "[7,    50] loss: 0.162\n",
            "[7,    50] acc: 95.714\n",
            "[7,    55] loss: 0.161\n",
            "[7,    55] acc: 95.741\n",
            "[7,    60] loss: 0.165\n",
            "[7,    60] acc: 95.593\n",
            "[7,    65] loss: 0.158\n",
            "[7,    65] acc: 95.938\n",
            "[7,    70] loss: 0.153\n",
            "[7,    70] acc: 95.797\n",
            "[7,    75] loss: 0.153\n",
            "[7,    75] acc: 95.541\n",
            "[7,    80] loss: 0.150\n",
            "[7,    80] acc: 95.696\n",
            "[7,    85] loss: 0.144\n",
            "[7,    85] acc: 95.952\n",
            "[7,    90] loss: 0.143\n",
            "[7,    90] acc: 95.730\n",
            "[7,    95] loss: 0.142\n",
            "[7,    95] acc: 95.745\n",
            "[7,   100] loss: 0.140\n",
            "[7,   100] acc: 95.758\n",
            "[7,   105] loss: 0.139\n",
            "[7,   105] acc: 95.769\n",
            "[7,   110] loss: 0.137\n",
            "[7,   110] acc: 95.872\n",
            "[7,   115] loss: 0.135\n",
            "[7,   115] acc: 95.877\n",
            "[7,   120] loss: 0.132\n",
            "[7,   120] acc: 95.966\n",
            "Begin Evaluation for Current Epoch\n",
            "[7,     5] val loss: 0.085\n",
            "[7,     5] val acc: 98.000\n",
            "[7,    10] val loss: 0.117\n",
            "[7,    10] val acc: 95.000\n",
            "[7,    15] val loss: 0.101\n",
            "[7,    15] val acc: 96.000\n",
            "[7,    20] val loss: 0.088\n",
            "[7,    20] val acc: 96.500\n",
            "[7,    25] val loss: 0.084\n",
            "[7,    25] val acc: 96.800\n",
            "[7,    30] val loss: 0.078\n",
            "[7,    30] val acc: 97.000\n",
            "[7,    35] val loss: 0.086\n",
            "[7,    35] val acc: 96.571\n",
            "[7,    40] val loss: 0.088\n",
            "[7,    40] val acc: 96.500\n",
            "[7,    45] val loss: 0.092\n",
            "[7,    45] val acc: 96.222\n",
            "[7,    50] val loss: 0.090\n",
            "[7,    50] val acc: 96.200\n",
            "[7,    55] val loss: 0.088\n",
            "[7,    55] val acc: 96.000\n",
            "[7,    60] val loss: 0.087\n",
            "[7,    60] val acc: 96.167\n",
            "[7,    65] val loss: 0.088\n",
            "[7,    65] val acc: 96.154\n",
            "[7,    70] val loss: 0.087\n",
            "[7,    70] val acc: 96.000\n",
            "[7,    75] val loss: 0.088\n",
            "[7,    75] val acc: 96.000\n",
            "[7,    80] val loss: 0.089\n",
            "[7,    80] val acc: 96.000\n",
            "[7,    85] val loss: 0.088\n",
            "[7,    85] val acc: 96.000\n",
            "[7,    90] val loss: 0.088\n",
            "[7,    90] val acc: 96.111\n",
            "[7,    95] val loss: 0.089\n",
            "[7,    95] val acc: 96.000\n",
            "[7,   100] val loss: 0.092\n",
            "[7,   100] val acc: 96.000\n",
            "[7,   105] val loss: 0.092\n",
            "[7,   105] val acc: 96.000\n",
            "[7,   110] val loss: 0.095\n",
            "[7,   110] val acc: 95.727\n",
            "[7,   115] val loss: 0.095\n",
            "[7,   115] val acc: 95.739\n",
            "[7,   120] val loss: 0.095\n",
            "[7,   120] val acc: 95.750\n",
            "Epoch 07: | Train Loss: 0.13131 | Val Loss: 0.09471 | Train Acc: 95.167| Val Acc: 95.750\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[8,     5] loss: 0.103\n",
            "[8,     5] acc: 117.500\n",
            "[8,    10] loss: 0.082\n",
            "[8,    10] acc: 106.667\n",
            "[8,    15] loss: 0.073\n",
            "[8,    15] acc: 103.571\n",
            "[8,    20] loss: 0.065\n",
            "[8,    20] acc: 102.105\n",
            "[8,    25] loss: 0.073\n",
            "[8,    25] acc: 100.833\n",
            "[8,    30] loss: 0.079\n",
            "[8,    30] acc: 99.655\n",
            "[8,    35] loss: 0.077\n",
            "[8,    35] acc: 99.118\n",
            "[8,    40] loss: 0.076\n",
            "[8,    40] acc: 98.974\n",
            "[8,    45] loss: 0.071\n",
            "[8,    45] acc: 99.091\n",
            "[8,    50] loss: 0.075\n",
            "[8,    50] acc: 98.776\n",
            "[8,    55] loss: 0.073\n",
            "[8,    55] acc: 98.889\n",
            "[8,    60] loss: 0.069\n",
            "[8,    60] acc: 98.983\n",
            "[8,    65] loss: 0.071\n",
            "[8,    65] acc: 98.906\n",
            "[8,    70] loss: 0.070\n",
            "[8,    70] acc: 98.841\n",
            "[8,    75] loss: 0.067\n",
            "[8,    75] acc: 98.919\n",
            "[8,    80] loss: 0.064\n",
            "[8,    80] acc: 98.987\n",
            "[8,    85] loss: 0.064\n",
            "[8,    85] acc: 98.929\n",
            "[8,    90] loss: 0.064\n",
            "[8,    90] acc: 98.876\n",
            "[8,    95] loss: 0.062\n",
            "[8,    95] acc: 98.830\n",
            "[8,   100] loss: 0.062\n",
            "[8,   100] acc: 98.889\n",
            "[8,   105] loss: 0.060\n",
            "[8,   105] acc: 98.942\n",
            "[8,   110] loss: 0.060\n",
            "[8,   110] acc: 98.899\n",
            "[8,   115] loss: 0.060\n",
            "[8,   115] acc: 98.860\n",
            "[8,   120] loss: 0.061\n",
            "[8,   120] acc: 98.824\n",
            "Begin Evaluation for Current Epoch\n",
            "[8,     5] val loss: 0.012\n",
            "[8,     5] val acc: 100.000\n",
            "[8,    10] val loss: 0.032\n",
            "[8,    10] val acc: 100.000\n",
            "[8,    15] val loss: 0.026\n",
            "[8,    15] val acc: 100.000\n",
            "[8,    20] val loss: 0.027\n",
            "[8,    20] val acc: 99.500\n",
            "[8,    25] val loss: 0.028\n",
            "[8,    25] val acc: 99.600\n",
            "[8,    30] val loss: 0.027\n",
            "[8,    30] val acc: 99.667\n",
            "[8,    35] val loss: 0.026\n",
            "[8,    35] val acc: 99.714\n",
            "[8,    40] val loss: 0.027\n",
            "[8,    40] val acc: 99.500\n",
            "[8,    45] val loss: 0.031\n",
            "[8,    45] val acc: 99.333\n",
            "[8,    50] val loss: 0.030\n",
            "[8,    50] val acc: 99.400\n",
            "[8,    55] val loss: 0.036\n",
            "[8,    55] val acc: 99.091\n",
            "[8,    60] val loss: 0.035\n",
            "[8,    60] val acc: 99.167\n",
            "[8,    65] val loss: 0.036\n",
            "[8,    65] val acc: 99.077\n",
            "[8,    70] val loss: 0.037\n",
            "[8,    70] val acc: 99.000\n",
            "[8,    75] val loss: 0.037\n",
            "[8,    75] val acc: 99.067\n",
            "[8,    80] val loss: 0.038\n",
            "[8,    80] val acc: 99.000\n",
            "[8,    85] val loss: 0.037\n",
            "[8,    85] val acc: 99.059\n",
            "[8,    90] val loss: 0.036\n",
            "[8,    90] val acc: 99.000\n",
            "[8,    95] val loss: 0.035\n",
            "[8,    95] val acc: 99.053\n",
            "[8,   100] val loss: 0.034\n",
            "[8,   100] val acc: 99.100\n",
            "[8,   105] val loss: 0.033\n",
            "[8,   105] val acc: 99.143\n",
            "[8,   110] val loss: 0.033\n",
            "[8,   110] val acc: 99.091\n",
            "[8,   115] val loss: 0.032\n",
            "[8,   115] val acc: 99.130\n",
            "[8,   120] val loss: 0.033\n",
            "[8,   120] val acc: 99.083\n",
            "Epoch 08: | Train Loss: 0.06004 | Val Loss: 0.03283 | Train Acc: 98.000| Val Acc: 99.083\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[9,     5] loss: 0.053\n",
            "[9,     5] acc: 122.500\n",
            "[9,    10] loss: 0.070\n",
            "[9,    10] acc: 107.778\n",
            "[9,    15] loss: 0.072\n",
            "[9,    15] acc: 104.286\n",
            "[9,    20] loss: 0.056\n",
            "[9,    20] acc: 103.158\n",
            "[9,    25] loss: 0.049\n",
            "[9,    25] acc: 102.500\n",
            "[9,    30] loss: 0.045\n",
            "[9,    30] acc: 102.069\n",
            "[9,    35] loss: 0.040\n",
            "[9,    35] acc: 101.765\n",
            "[9,    40] loss: 0.037\n",
            "[9,    40] acc: 101.538\n",
            "[9,    45] loss: 0.034\n",
            "[9,    45] acc: 101.364\n",
            "[9,    50] loss: 0.033\n",
            "[9,    50] acc: 101.224\n",
            "[9,    55] loss: 0.031\n",
            "[9,    55] acc: 101.111\n",
            "[9,    60] loss: 0.031\n",
            "[9,    60] acc: 101.017\n",
            "[9,    65] loss: 0.029\n",
            "[9,    65] acc: 100.938\n",
            "[9,    70] loss: 0.029\n",
            "[9,    70] acc: 100.870\n",
            "[9,    75] loss: 0.029\n",
            "[9,    75] acc: 100.811\n",
            "[9,    80] loss: 0.030\n",
            "[9,    80] acc: 100.633\n",
            "[9,    85] loss: 0.032\n",
            "[9,    85] acc: 100.357\n",
            "[9,    90] loss: 0.032\n",
            "[9,    90] acc: 100.112\n",
            "[9,    95] loss: 0.034\n",
            "[9,    95] acc: 100.000\n",
            "[9,   100] loss: 0.034\n",
            "[9,   100] acc: 100.000\n",
            "[9,   105] loss: 0.033\n",
            "[9,   105] acc: 100.000\n",
            "[9,   110] loss: 0.035\n",
            "[9,   110] acc: 99.908\n",
            "[9,   115] loss: 0.036\n",
            "[9,   115] acc: 99.825\n",
            "[9,   120] loss: 0.035\n",
            "[9,   120] acc: 99.832\n",
            "Begin Evaluation for Current Epoch\n",
            "[9,     5] val loss: 0.053\n",
            "[9,     5] val acc: 98.000\n",
            "[9,    10] val loss: 0.033\n",
            "[9,    10] val acc: 99.000\n",
            "[9,    15] val loss: 0.030\n",
            "[9,    15] val acc: 99.333\n",
            "[9,    20] val loss: 0.029\n",
            "[9,    20] val acc: 99.500\n",
            "[9,    25] val loss: 0.025\n",
            "[9,    25] val acc: 99.600\n",
            "[9,    30] val loss: 0.022\n",
            "[9,    30] val acc: 99.667\n",
            "[9,    35] val loss: 0.026\n",
            "[9,    35] val acc: 99.429\n",
            "[9,    40] val loss: 0.025\n",
            "[9,    40] val acc: 99.500\n",
            "[9,    45] val loss: 0.024\n",
            "[9,    45] val acc: 99.556\n",
            "[9,    50] val loss: 0.023\n",
            "[9,    50] val acc: 99.600\n",
            "[9,    55] val loss: 0.022\n",
            "[9,    55] val acc: 99.636\n",
            "[9,    60] val loss: 0.022\n",
            "[9,    60] val acc: 99.667\n",
            "[9,    65] val loss: 0.021\n",
            "[9,    65] val acc: 99.692\n",
            "[9,    70] val loss: 0.020\n",
            "[9,    70] val acc: 99.714\n",
            "[9,    75] val loss: 0.020\n",
            "[9,    75] val acc: 99.733\n",
            "[9,    80] val loss: 0.021\n",
            "[9,    80] val acc: 99.750\n",
            "[9,    85] val loss: 0.020\n",
            "[9,    85] val acc: 99.765\n",
            "[9,    90] val loss: 0.020\n",
            "[9,    90] val acc: 99.778\n",
            "[9,    95] val loss: 0.020\n",
            "[9,    95] val acc: 99.789\n",
            "[9,   100] val loss: 0.020\n",
            "[9,   100] val acc: 99.800\n",
            "[9,   105] val loss: 0.019\n",
            "[9,   105] val acc: 99.810\n",
            "[9,   110] val loss: 0.019\n",
            "[9,   110] val acc: 99.818\n",
            "[9,   115] val loss: 0.020\n",
            "[9,   115] val acc: 99.739\n",
            "[9,   120] val loss: 0.019\n",
            "[9,   120] val acc: 99.750\n",
            "Epoch 09: | Train Loss: 0.03450 | Val Loss: 0.01941 | Train Acc: 99.000| Val Acc: 99.750\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[10,     5] loss: 0.015\n",
            "[10,     5] acc: 125.000\n",
            "[10,    10] loss: 0.043\n",
            "[10,    10] acc: 110.000\n",
            "[10,    15] loss: 0.035\n",
            "[10,    15] acc: 106.429\n",
            "[10,    20] loss: 0.029\n",
            "[10,    20] acc: 104.737\n",
            "[10,    25] loss: 0.025\n",
            "[10,    25] acc: 103.750\n",
            "[10,    30] loss: 0.024\n",
            "[10,    30] acc: 103.103\n",
            "[10,    35] loss: 0.022\n",
            "[10,    35] acc: 102.647\n",
            "[10,    40] loss: 0.022\n",
            "[10,    40] acc: 102.308\n",
            "[10,    45] loss: 0.021\n",
            "[10,    45] acc: 102.045\n",
            "[10,    50] loss: 0.020\n",
            "[10,    50] acc: 101.837\n",
            "[10,    55] loss: 0.020\n",
            "[10,    55] acc: 101.481\n",
            "[10,    60] loss: 0.019\n",
            "[10,    60] acc: 101.356\n",
            "[10,    65] loss: 0.018\n",
            "[10,    65] acc: 101.250\n",
            "[10,    70] loss: 0.018\n",
            "[10,    70] acc: 101.159\n",
            "[10,    75] loss: 0.017\n",
            "[10,    75] acc: 101.081\n",
            "[10,    80] loss: 0.018\n",
            "[10,    80] acc: 101.013\n",
            "[10,    85] loss: 0.017\n",
            "[10,    85] acc: 100.952\n",
            "[10,    90] loss: 0.017\n",
            "[10,    90] acc: 100.899\n",
            "[10,    95] loss: 0.017\n",
            "[10,    95] acc: 100.851\n",
            "[10,   100] loss: 0.016\n",
            "[10,   100] acc: 100.808\n",
            "[10,   105] loss: 0.016\n",
            "[10,   105] acc: 100.769\n",
            "[10,   110] loss: 0.015\n",
            "[10,   110] acc: 100.734\n",
            "[10,   115] loss: 0.015\n",
            "[10,   115] acc: 100.702\n",
            "[10,   120] loss: 0.015\n",
            "[10,   120] acc: 100.672\n",
            "Begin Evaluation for Current Epoch\n",
            "[10,     5] val loss: 0.005\n",
            "[10,     5] val acc: 100.000\n",
            "[10,    10] val loss: 0.005\n",
            "[10,    10] val acc: 100.000\n",
            "[10,    15] val loss: 0.005\n",
            "[10,    15] val acc: 100.000\n",
            "[10,    20] val loss: 0.005\n",
            "[10,    20] val acc: 100.000\n",
            "[10,    25] val loss: 0.005\n",
            "[10,    25] val acc: 100.000\n",
            "[10,    30] val loss: 0.005\n",
            "[10,    30] val acc: 100.000\n",
            "[10,    35] val loss: 0.005\n",
            "[10,    35] val acc: 100.000\n",
            "[10,    40] val loss: 0.005\n",
            "[10,    40] val acc: 100.000\n",
            "[10,    45] val loss: 0.005\n",
            "[10,    45] val acc: 100.000\n",
            "[10,    50] val loss: 0.004\n",
            "[10,    50] val acc: 100.000\n",
            "[10,    55] val loss: 0.004\n",
            "[10,    55] val acc: 100.000\n",
            "[10,    60] val loss: 0.004\n",
            "[10,    60] val acc: 100.000\n",
            "[10,    65] val loss: 0.004\n",
            "[10,    65] val acc: 100.000\n",
            "[10,    70] val loss: 0.004\n",
            "[10,    70] val acc: 100.000\n",
            "[10,    75] val loss: 0.004\n",
            "[10,    75] val acc: 100.000\n",
            "[10,    80] val loss: 0.005\n",
            "[10,    80] val acc: 100.000\n",
            "[10,    85] val loss: 0.005\n",
            "[10,    85] val acc: 100.000\n",
            "[10,    90] val loss: 0.006\n",
            "[10,    90] val acc: 100.000\n",
            "[10,    95] val loss: 0.005\n",
            "[10,    95] val acc: 100.000\n",
            "[10,   100] val loss: 0.006\n",
            "[10,   100] val acc: 100.000\n",
            "[10,   105] val loss: 0.005\n",
            "[10,   105] val acc: 100.000\n",
            "[10,   110] val loss: 0.005\n",
            "[10,   110] val acc: 100.000\n",
            "[10,   115] val loss: 0.005\n",
            "[10,   115] val acc: 100.000\n",
            "[10,   120] val loss: 0.006\n",
            "[10,   120] val acc: 100.000\n",
            "Epoch 10: | Train Loss: 0.01448 | Val Loss: 0.00550 | Train Acc: 99.833| Val Acc: 100.000\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[11,     5] loss: 0.005\n",
            "[11,     5] acc: 125.000\n",
            "[11,    10] loss: 0.005\n",
            "[11,    10] acc: 111.111\n",
            "[11,    15] loss: 0.007\n",
            "[11,    15] acc: 107.143\n",
            "[11,    20] loss: 0.007\n",
            "[11,    20] acc: 105.263\n",
            "[11,    25] loss: 0.008\n",
            "[11,    25] acc: 104.167\n",
            "[11,    30] loss: 0.009\n",
            "[11,    30] acc: 103.448\n",
            "[11,    35] loss: 0.008\n",
            "[11,    35] acc: 102.941\n",
            "[11,    40] loss: 0.009\n",
            "[11,    40] acc: 102.564\n",
            "[11,    45] loss: 0.008\n",
            "[11,    45] acc: 102.273\n",
            "[11,    50] loss: 0.008\n",
            "[11,    50] acc: 102.041\n",
            "[11,    55] loss: 0.008\n",
            "[11,    55] acc: 101.852\n",
            "[11,    60] loss: 0.008\n",
            "[11,    60] acc: 101.695\n",
            "[11,    65] loss: 0.008\n",
            "[11,    65] acc: 101.562\n",
            "[11,    70] loss: 0.007\n",
            "[11,    70] acc: 101.449\n",
            "[11,    75] loss: 0.007\n",
            "[11,    75] acc: 101.351\n",
            "[11,    80] loss: 0.007\n",
            "[11,    80] acc: 101.266\n",
            "[11,    85] loss: 0.007\n",
            "[11,    85] acc: 101.190\n",
            "[11,    90] loss: 0.007\n",
            "[11,    90] acc: 101.124\n",
            "[11,    95] loss: 0.006\n",
            "[11,    95] acc: 101.064\n",
            "[11,   100] loss: 0.006\n",
            "[11,   100] acc: 101.010\n",
            "[11,   105] loss: 0.006\n",
            "[11,   105] acc: 100.962\n",
            "[11,   110] loss: 0.006\n",
            "[11,   110] acc: 100.917\n",
            "[11,   115] loss: 0.006\n",
            "[11,   115] acc: 100.877\n",
            "[11,   120] loss: 0.006\n",
            "[11,   120] acc: 100.840\n",
            "Begin Evaluation for Current Epoch\n",
            "[11,     5] val loss: 0.003\n",
            "[11,     5] val acc: 100.000\n",
            "[11,    10] val loss: 0.005\n",
            "[11,    10] val acc: 100.000\n",
            "[11,    15] val loss: 0.004\n",
            "[11,    15] val acc: 100.000\n",
            "[11,    20] val loss: 0.004\n",
            "[11,    20] val acc: 100.000\n",
            "[11,    25] val loss: 0.004\n",
            "[11,    25] val acc: 100.000\n",
            "[11,    30] val loss: 0.004\n",
            "[11,    30] val acc: 100.000\n",
            "[11,    35] val loss: 0.004\n",
            "[11,    35] val acc: 100.000\n",
            "[11,    40] val loss: 0.003\n",
            "[11,    40] val acc: 100.000\n",
            "[11,    45] val loss: 0.003\n",
            "[11,    45] val acc: 100.000\n",
            "[11,    50] val loss: 0.003\n",
            "[11,    50] val acc: 100.000\n",
            "[11,    55] val loss: 0.003\n",
            "[11,    55] val acc: 100.000\n",
            "[11,    60] val loss: 0.003\n",
            "[11,    60] val acc: 100.000\n",
            "[11,    65] val loss: 0.003\n",
            "[11,    65] val acc: 100.000\n",
            "[11,    70] val loss: 0.003\n",
            "[11,    70] val acc: 100.000\n",
            "[11,    75] val loss: 0.003\n",
            "[11,    75] val acc: 100.000\n",
            "[11,    80] val loss: 0.003\n",
            "[11,    80] val acc: 100.000\n",
            "[11,    85] val loss: 0.003\n",
            "[11,    85] val acc: 100.000\n",
            "[11,    90] val loss: 0.003\n",
            "[11,    90] val acc: 100.000\n",
            "[11,    95] val loss: 0.003\n",
            "[11,    95] val acc: 100.000\n",
            "[11,   100] val loss: 0.003\n",
            "[11,   100] val acc: 100.000\n",
            "[11,   105] val loss: 0.003\n",
            "[11,   105] val acc: 100.000\n",
            "[11,   110] val loss: 0.003\n",
            "[11,   110] val acc: 100.000\n",
            "[11,   115] val loss: 0.003\n",
            "[11,   115] val acc: 100.000\n",
            "[11,   120] val loss: 0.003\n",
            "[11,   120] val acc: 100.000\n",
            "Epoch 11: | Train Loss: 0.00576 | Val Loss: 0.00272 | Train Acc: 100.000| Val Acc: 100.000\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[12,     5] loss: 0.005\n",
            "[12,     5] acc: 125.000\n",
            "[12,    10] loss: 0.004\n",
            "[12,    10] acc: 111.111\n",
            "[12,    15] loss: 0.003\n",
            "[12,    15] acc: 107.143\n",
            "[12,    20] loss: 0.003\n",
            "[12,    20] acc: 105.263\n",
            "[12,    25] loss: 0.003\n",
            "[12,    25] acc: 104.167\n",
            "[12,    30] loss: 0.003\n",
            "[12,    30] acc: 103.448\n",
            "[12,    35] loss: 0.003\n",
            "[12,    35] acc: 102.941\n",
            "[12,    40] loss: 0.002\n",
            "[12,    40] acc: 102.564\n",
            "[12,    45] loss: 0.003\n",
            "[12,    45] acc: 102.273\n",
            "[12,    50] loss: 0.002\n",
            "[12,    50] acc: 102.041\n",
            "[12,    55] loss: 0.003\n",
            "[12,    55] acc: 101.852\n",
            "[12,    60] loss: 0.002\n",
            "[12,    60] acc: 101.695\n",
            "[12,    65] loss: 0.003\n",
            "[12,    65] acc: 101.562\n",
            "[12,    70] loss: 0.002\n",
            "[12,    70] acc: 101.449\n",
            "[12,    75] loss: 0.002\n",
            "[12,    75] acc: 101.351\n",
            "[12,    80] loss: 0.002\n",
            "[12,    80] acc: 101.266\n",
            "[12,    85] loss: 0.002\n",
            "[12,    85] acc: 101.190\n",
            "[12,    90] loss: 0.002\n",
            "[12,    90] acc: 101.124\n",
            "[12,    95] loss: 0.002\n",
            "[12,    95] acc: 101.064\n",
            "[12,   100] loss: 0.002\n",
            "[12,   100] acc: 101.010\n",
            "[12,   105] loss: 0.002\n",
            "[12,   105] acc: 100.962\n",
            "[12,   110] loss: 0.002\n",
            "[12,   110] acc: 100.917\n",
            "[12,   115] loss: 0.002\n",
            "[12,   115] acc: 100.877\n",
            "[12,   120] loss: 0.002\n",
            "[12,   120] acc: 100.840\n",
            "Begin Evaluation for Current Epoch\n",
            "[12,     5] val loss: 0.001\n",
            "[12,     5] val acc: 100.000\n",
            "[12,    10] val loss: 0.001\n",
            "[12,    10] val acc: 100.000\n",
            "[12,    15] val loss: 0.001\n",
            "[12,    15] val acc: 100.000\n",
            "[12,    20] val loss: 0.001\n",
            "[12,    20] val acc: 100.000\n",
            "[12,    25] val loss: 0.002\n",
            "[12,    25] val acc: 100.000\n",
            "[12,    30] val loss: 0.001\n",
            "[12,    30] val acc: 100.000\n",
            "[12,    35] val loss: 0.001\n",
            "[12,    35] val acc: 100.000\n",
            "[12,    40] val loss: 0.001\n",
            "[12,    40] val acc: 100.000\n",
            "[12,    45] val loss: 0.002\n",
            "[12,    45] val acc: 100.000\n",
            "[12,    50] val loss: 0.002\n",
            "[12,    50] val acc: 100.000\n",
            "[12,    55] val loss: 0.002\n",
            "[12,    55] val acc: 100.000\n",
            "[12,    60] val loss: 0.002\n",
            "[12,    60] val acc: 100.000\n",
            "[12,    65] val loss: 0.001\n",
            "[12,    65] val acc: 100.000\n",
            "[12,    70] val loss: 0.001\n",
            "[12,    70] val acc: 100.000\n",
            "[12,    75] val loss: 0.001\n",
            "[12,    75] val acc: 100.000\n",
            "[12,    80] val loss: 0.002\n",
            "[12,    80] val acc: 100.000\n",
            "[12,    85] val loss: 0.002\n",
            "[12,    85] val acc: 100.000\n",
            "[12,    90] val loss: 0.001\n",
            "[12,    90] val acc: 100.000\n",
            "[12,    95] val loss: 0.001\n",
            "[12,    95] val acc: 100.000\n",
            "[12,   100] val loss: 0.001\n",
            "[12,   100] val acc: 100.000\n",
            "[12,   105] val loss: 0.001\n",
            "[12,   105] val acc: 100.000\n",
            "[12,   110] val loss: 0.001\n",
            "[12,   110] val acc: 100.000\n",
            "[12,   115] val loss: 0.002\n",
            "[12,   115] val acc: 100.000\n",
            "[12,   120] val loss: 0.002\n",
            "[12,   120] val acc: 100.000\n",
            "Epoch 12: | Train Loss: 0.00214 | Val Loss: 0.00151 | Train Acc: 100.000| Val Acc: 100.000\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[13,     5] loss: 0.003\n",
            "[13,     5] acc: 125.000\n",
            "[13,    10] loss: 0.002\n",
            "[13,    10] acc: 111.111\n",
            "[13,    15] loss: 0.002\n",
            "[13,    15] acc: 107.143\n",
            "[13,    20] loss: 0.001\n",
            "[13,    20] acc: 105.263\n",
            "[13,    25] loss: 0.002\n",
            "[13,    25] acc: 104.167\n",
            "[13,    30] loss: 0.001\n",
            "[13,    30] acc: 103.448\n",
            "[13,    35] loss: 0.002\n",
            "[13,    35] acc: 102.941\n",
            "[13,    40] loss: 0.002\n",
            "[13,    40] acc: 102.564\n",
            "[13,    45] loss: 0.001\n",
            "[13,    45] acc: 102.273\n",
            "[13,    50] loss: 0.002\n",
            "[13,    50] acc: 102.041\n",
            "[13,    55] loss: 0.001\n",
            "[13,    55] acc: 101.852\n",
            "[13,    60] loss: 0.001\n",
            "[13,    60] acc: 101.695\n",
            "[13,    65] loss: 0.001\n",
            "[13,    65] acc: 101.562\n",
            "[13,    70] loss: 0.001\n",
            "[13,    70] acc: 101.449\n",
            "[13,    75] loss: 0.001\n",
            "[13,    75] acc: 101.351\n",
            "[13,    80] loss: 0.001\n",
            "[13,    80] acc: 101.266\n",
            "[13,    85] loss: 0.001\n",
            "[13,    85] acc: 101.190\n",
            "[13,    90] loss: 0.001\n",
            "[13,    90] acc: 101.124\n",
            "[13,    95] loss: 0.001\n",
            "[13,    95] acc: 101.064\n",
            "[13,   100] loss: 0.001\n",
            "[13,   100] acc: 101.010\n",
            "[13,   105] loss: 0.001\n",
            "[13,   105] acc: 100.962\n",
            "[13,   110] loss: 0.001\n",
            "[13,   110] acc: 100.917\n",
            "[13,   115] loss: 0.001\n",
            "[13,   115] acc: 100.877\n",
            "[13,   120] loss: 0.001\n",
            "[13,   120] acc: 100.840\n",
            "Begin Evaluation for Current Epoch\n",
            "[13,     5] val loss: 0.001\n",
            "[13,     5] val acc: 100.000\n",
            "[13,    10] val loss: 0.001\n",
            "[13,    10] val acc: 100.000\n",
            "[13,    15] val loss: 0.001\n",
            "[13,    15] val acc: 100.000\n",
            "[13,    20] val loss: 0.001\n",
            "[13,    20] val acc: 100.000\n",
            "[13,    25] val loss: 0.001\n",
            "[13,    25] val acc: 100.000\n",
            "[13,    30] val loss: 0.001\n",
            "[13,    30] val acc: 100.000\n",
            "[13,    35] val loss: 0.001\n",
            "[13,    35] val acc: 100.000\n",
            "[13,    40] val loss: 0.001\n",
            "[13,    40] val acc: 100.000\n",
            "[13,    45] val loss: 0.001\n",
            "[13,    45] val acc: 100.000\n",
            "[13,    50] val loss: 0.001\n",
            "[13,    50] val acc: 100.000\n",
            "[13,    55] val loss: 0.001\n",
            "[13,    55] val acc: 100.000\n",
            "[13,    60] val loss: 0.001\n",
            "[13,    60] val acc: 100.000\n",
            "[13,    65] val loss: 0.001\n",
            "[13,    65] val acc: 100.000\n",
            "[13,    70] val loss: 0.001\n",
            "[13,    70] val acc: 100.000\n",
            "[13,    75] val loss: 0.001\n",
            "[13,    75] val acc: 100.000\n",
            "[13,    80] val loss: 0.001\n",
            "[13,    80] val acc: 100.000\n",
            "[13,    85] val loss: 0.001\n",
            "[13,    85] val acc: 100.000\n",
            "[13,    90] val loss: 0.001\n",
            "[13,    90] val acc: 100.000\n",
            "[13,    95] val loss: 0.001\n",
            "[13,    95] val acc: 100.000\n",
            "[13,   100] val loss: 0.001\n",
            "[13,   100] val acc: 100.000\n",
            "[13,   105] val loss: 0.001\n",
            "[13,   105] val acc: 100.000\n",
            "[13,   110] val loss: 0.001\n",
            "[13,   110] val acc: 100.000\n",
            "[13,   115] val loss: 0.001\n",
            "[13,   115] val acc: 100.000\n",
            "[13,   120] val loss: 0.001\n",
            "[13,   120] val acc: 100.000\n",
            "Epoch 13: | Train Loss: 0.00140 | Val Loss: 0.00114 | Train Acc: 100.000| Val Acc: 100.000\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[14,     5] loss: 0.002\n",
            "[14,     5] acc: 125.000\n",
            "[14,    10] loss: 0.001\n",
            "[14,    10] acc: 111.111\n",
            "[14,    15] loss: 0.001\n",
            "[14,    15] acc: 107.143\n",
            "[14,    20] loss: 0.001\n",
            "[14,    20] acc: 105.263\n",
            "[14,    25] loss: 0.001\n",
            "[14,    25] acc: 104.167\n",
            "[14,    30] loss: 0.001\n",
            "[14,    30] acc: 103.448\n",
            "[14,    35] loss: 0.001\n",
            "[14,    35] acc: 102.941\n",
            "[14,    40] loss: 0.001\n",
            "[14,    40] acc: 102.564\n",
            "[14,    45] loss: 0.001\n",
            "[14,    45] acc: 102.273\n",
            "[14,    50] loss: 0.001\n",
            "[14,    50] acc: 102.041\n",
            "[14,    55] loss: 0.001\n",
            "[14,    55] acc: 101.852\n",
            "[14,    60] loss: 0.001\n",
            "[14,    60] acc: 101.695\n",
            "[14,    65] loss: 0.001\n",
            "[14,    65] acc: 101.562\n",
            "[14,    70] loss: 0.001\n",
            "[14,    70] acc: 101.449\n",
            "[14,    75] loss: 0.001\n",
            "[14,    75] acc: 101.351\n",
            "[14,    80] loss: 0.001\n",
            "[14,    80] acc: 101.266\n",
            "[14,    85] loss: 0.001\n",
            "[14,    85] acc: 101.190\n",
            "[14,    90] loss: 0.001\n",
            "[14,    90] acc: 101.124\n",
            "[14,    95] loss: 0.001\n",
            "[14,    95] acc: 101.064\n",
            "[14,   100] loss: 0.001\n",
            "[14,   100] acc: 101.010\n",
            "[14,   105] loss: 0.001\n",
            "[14,   105] acc: 100.962\n",
            "[14,   110] loss: 0.001\n",
            "[14,   110] acc: 100.917\n",
            "[14,   115] loss: 0.001\n",
            "[14,   115] acc: 100.877\n",
            "[14,   120] loss: 0.001\n",
            "[14,   120] acc: 100.840\n",
            "Begin Evaluation for Current Epoch\n",
            "[14,     5] val loss: 0.001\n",
            "[14,     5] val acc: 100.000\n",
            "[14,    10] val loss: 0.001\n",
            "[14,    10] val acc: 100.000\n",
            "[14,    15] val loss: 0.001\n",
            "[14,    15] val acc: 100.000\n",
            "[14,    20] val loss: 0.001\n",
            "[14,    20] val acc: 100.000\n",
            "[14,    25] val loss: 0.001\n",
            "[14,    25] val acc: 100.000\n",
            "[14,    30] val loss: 0.001\n",
            "[14,    30] val acc: 100.000\n",
            "[14,    35] val loss: 0.001\n",
            "[14,    35] val acc: 100.000\n",
            "[14,    40] val loss: 0.001\n",
            "[14,    40] val acc: 100.000\n",
            "[14,    45] val loss: 0.001\n",
            "[14,    45] val acc: 100.000\n",
            "[14,    50] val loss: 0.001\n",
            "[14,    50] val acc: 100.000\n",
            "[14,    55] val loss: 0.001\n",
            "[14,    55] val acc: 100.000\n",
            "[14,    60] val loss: 0.001\n",
            "[14,    60] val acc: 100.000\n",
            "[14,    65] val loss: 0.001\n",
            "[14,    65] val acc: 100.000\n",
            "[14,    70] val loss: 0.001\n",
            "[14,    70] val acc: 100.000\n",
            "[14,    75] val loss: 0.001\n",
            "[14,    75] val acc: 100.000\n",
            "[14,    80] val loss: 0.001\n",
            "[14,    80] val acc: 100.000\n",
            "[14,    85] val loss: 0.001\n",
            "[14,    85] val acc: 100.000\n",
            "[14,    90] val loss: 0.001\n",
            "[14,    90] val acc: 100.000\n",
            "[14,    95] val loss: 0.001\n",
            "[14,    95] val acc: 100.000\n",
            "[14,   100] val loss: 0.001\n",
            "[14,   100] val acc: 100.000\n",
            "[14,   105] val loss: 0.001\n",
            "[14,   105] val acc: 100.000\n",
            "[14,   110] val loss: 0.001\n",
            "[14,   110] val acc: 100.000\n",
            "[14,   115] val loss: 0.001\n",
            "[14,   115] val acc: 100.000\n",
            "[14,   120] val loss: 0.001\n",
            "[14,   120] val acc: 100.000\n",
            "Epoch 14: | Train Loss: 0.00107 | Val Loss: 0.00093 | Train Acc: 100.000| Val Acc: 100.000\n",
            "Starting Next Epoch\n",
            "Begin Training for Current Epoch\n",
            "[15,     5] loss: 0.001\n",
            "[15,     5] acc: 125.000\n",
            "[15,    10] loss: 0.001\n",
            "[15,    10] acc: 111.111\n",
            "[15,    15] loss: 0.001\n",
            "[15,    15] acc: 107.143\n",
            "[15,    20] loss: 0.001\n",
            "[15,    20] acc: 105.263\n",
            "[15,    25] loss: 0.001\n",
            "[15,    25] acc: 104.167\n",
            "[15,    30] loss: 0.001\n",
            "[15,    30] acc: 103.448\n",
            "[15,    35] loss: 0.001\n",
            "[15,    35] acc: 102.941\n",
            "[15,    40] loss: 0.001\n",
            "[15,    40] acc: 102.564\n",
            "[15,    45] loss: 0.001\n",
            "[15,    45] acc: 102.273\n",
            "[15,    50] loss: 0.001\n",
            "[15,    50] acc: 102.041\n",
            "[15,    55] loss: 0.001\n",
            "[15,    55] acc: 101.852\n",
            "[15,    60] loss: 0.001\n",
            "[15,    60] acc: 101.695\n",
            "[15,    65] loss: 0.001\n",
            "[15,    65] acc: 101.562\n",
            "[15,    70] loss: 0.001\n",
            "[15,    70] acc: 101.449\n",
            "[15,    75] loss: 0.001\n",
            "[15,    75] acc: 101.351\n",
            "[15,    80] loss: 0.001\n",
            "[15,    80] acc: 101.266\n",
            "[15,    85] loss: 0.001\n",
            "[15,    85] acc: 101.190\n",
            "[15,    90] loss: 0.001\n",
            "[15,    90] acc: 101.124\n",
            "[15,    95] loss: 0.001\n",
            "[15,    95] acc: 101.064\n",
            "[15,   100] loss: 0.001\n",
            "[15,   100] acc: 101.010\n",
            "[15,   105] loss: 0.001\n",
            "[15,   105] acc: 100.962\n",
            "[15,   110] loss: 0.001\n",
            "[15,   110] acc: 100.917\n",
            "[15,   115] loss: 0.001\n",
            "[15,   115] acc: 100.877\n",
            "[15,   120] loss: 0.001\n",
            "[15,   120] acc: 100.840\n",
            "Begin Evaluation for Current Epoch\n",
            "[15,     5] val loss: 0.001\n",
            "[15,     5] val acc: 100.000\n",
            "[15,    10] val loss: 0.001\n",
            "[15,    10] val acc: 100.000\n",
            "[15,    15] val loss: 0.001\n",
            "[15,    15] val acc: 100.000\n",
            "[15,    20] val loss: 0.001\n",
            "[15,    20] val acc: 100.000\n",
            "[15,    25] val loss: 0.001\n",
            "[15,    25] val acc: 100.000\n",
            "[15,    30] val loss: 0.001\n",
            "[15,    30] val acc: 100.000\n",
            "[15,    35] val loss: 0.001\n",
            "[15,    35] val acc: 100.000\n",
            "[15,    40] val loss: 0.001\n",
            "[15,    40] val acc: 100.000\n",
            "[15,    45] val loss: 0.001\n",
            "[15,    45] val acc: 100.000\n",
            "[15,    50] val loss: 0.001\n",
            "[15,    50] val acc: 100.000\n",
            "[15,    55] val loss: 0.001\n",
            "[15,    55] val acc: 100.000\n",
            "[15,    60] val loss: 0.001\n",
            "[15,    60] val acc: 100.000\n",
            "[15,    65] val loss: 0.001\n",
            "[15,    65] val acc: 100.000\n",
            "[15,    70] val loss: 0.001\n",
            "[15,    70] val acc: 100.000\n",
            "[15,    75] val loss: 0.001\n",
            "[15,    75] val acc: 100.000\n",
            "[15,    80] val loss: 0.001\n",
            "[15,    80] val acc: 100.000\n",
            "[15,    85] val loss: 0.001\n",
            "[15,    85] val acc: 100.000\n",
            "[15,    90] val loss: 0.001\n",
            "[15,    90] val acc: 100.000\n",
            "[15,    95] val loss: 0.001\n",
            "[15,    95] val acc: 100.000\n",
            "[15,   100] val loss: 0.001\n",
            "[15,   100] val acc: 100.000\n",
            "[15,   105] val loss: 0.001\n",
            "[15,   105] val acc: 100.000\n",
            "[15,   110] val loss: 0.001\n",
            "[15,   110] val acc: 100.000\n",
            "[15,   115] val loss: 0.001\n",
            "[15,   115] val acc: 100.000\n",
            "[15,   120] val loss: 0.001\n",
            "[15,   120] val acc: 100.000\n",
            "Epoch 15: | Train Loss: 0.00088 | Val Loss: 0.00078 | Train Acc: 100.000| Val Acc: 100.000\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "zKMGUrqUHb68",
        "outputId": "f0e820f8-38c1-4b15-867a-1198ef62f163"
      },
      "source": [
        "test_data = DEM_Dataset(args.num_samples, layers = args.layer_types, train = False)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size,shuffle=True)\n",
        "\n",
        "# Sketch of what to do with test data, from https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89\n",
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # for i, val_data in enumerate(testloader,0):\n",
        "    #           val_input, val_label = val_data\n",
        "    #           print(val_label)\n",
        "    for X_batch in testloader:\n",
        "        # X_batch = X_batch.to(device)\n",
        "        test_input, test_label = X_batch\n",
        "        y_test_pred = model(X_batch)\n",
        "        # This may need to be softmax, since using loss function same as https://towardsdatascience.com/pytorch-vision-binary-image-classification-d9a227705cf9\n",
        "        # See \"architecture\" section of this source for explanation\n",
        "        y_test_pred = torch.sigmoid(y_test_pred)\n",
        "        y_pred_tag = torch.round(y_test_pred)\n",
        "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
        "        \n",
        "\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-eb00e63b29f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# X_batch = X_batch.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# This may need to be softmax, since using loss function same as https://towardsdatascience.com/pytorch-vision-binary-image-classification-d9a227705cf9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# See \"architecture\" section of this source for explanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c59e16019120>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aV3JM4BIY-1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vhkMczUPZRI"
      },
      "source": [
        "# load model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu-uXv9cPazd"
      },
      "source": [
        "m_dict = torch.load('/content/drive/MyDrive/Landslide Project/Landslide Prediction CNN.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXAFBp-sSEmD",
        "outputId": "31052799-5aae-473e-9cee-65f53e5426e4"
      },
      "source": [
        "model = CNN_Topo()\n",
        "model = model.double()\n",
        "model.load_state_dict(m_dict)\n",
        "device = torch.device(args.device)\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_Topo(\n",
              "  (conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=51200, out_features=500, bias=True)\n",
              "  (fc2): Linear(in_features=500, out_features=50, bias=True)\n",
              "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pKMqTw9R0o2",
        "outputId": "7f088788-415f-4550-b872-d174437f6cad"
      },
      "source": [
        "print(type(model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.CNN_Topo'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12U7fw2uTRQl"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7IqnjxqQzWM",
        "outputId": "86bf5da3-17ca-4c95-d135-5e81da5292e0"
      },
      "source": [
        "test_data = DEM_Dataset(200, layers = args.layer_types, train = False)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size,shuffle=True)\n",
        "accuracy = 0\n",
        "\n",
        "# Sketch of what to do with test data, from https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89\n",
        "y_pred_list = []\n",
        "y_labels_list = []\n",
        "with torch.no_grad():\n",
        "  # for i, val_data in enumerate(valloader,0):\n",
        "  #             val_input, val_label = val_data\n",
        "  #             # nan_to_num in dataset creation but nan's still appearing in inputs - not sure why\n",
        "  #             val_input[torch.isnan(val_input)] = 0\n",
        "  #             val_input, val_label = val_input.to(device), val_label.to(device)\n",
        "              \n",
        "  #             output = model(val_input)\n",
        "    for X_batch in testloader:\n",
        "        # X_batch = X_batch.to(device)\n",
        "        items, labels = X_batch\n",
        "        items, labels = items.to(device), labels.to(device)\n",
        "        # print(items)\n",
        "        # items = items.type(torch.DoubleTensor)\n",
        "        items = items.to(torch.double)\n",
        "        y_test_pred = model(items.double())\n",
        "\n",
        "        val_acc = binary_acc(y_test_pred, labels)\n",
        "        accuracy += val_acc\n",
        "        # This may need to be softmax, since using loss function same as https://towardsdatascience.com/pytorch-vision-binary-image-classification-d9a227705cf9\n",
        "        # See \"architecture\" section of this source for explanation\n",
        "\n",
        "        y_pred_tag = torch.log_softmax(y_test_pred, dim = 1)\n",
        "        _, y_pred_tags = torch.max(y_pred_tag, dim = 1)\n",
        "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
        "        y_labels_list.append(labels.numpy())\n",
        "      \n",
        "        # y_test_pred = torch.sigmoid(y_test_pred)\n",
        "        # y_pred_tag = torch.round(y_test_pred)\n",
        "                                 \n",
        "\n",
        "        # y_pred_list.append(y_pred_tag.cpu().numpy())\n",
        "        # y_labels_list.append(labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_pred_list = [a for a in y_pred_list]\n",
        "y_labels_list = [a for a in y_labels_list]\n",
        "\n",
        "y_preds = np.array(y_pred_list).flatten()\n",
        "y_labels = np.array(y_labels_list).flatten()\n",
        "print(y_preds)\n",
        "\n",
        "\n",
        "\n",
        "# for a, b in zip(y_pred_list, y_labels_list):\n",
        "#   print(a.flatten())\n",
        "#   print(type(b))\n",
        "#   y_preds.append(a.tolist())\n",
        "#   y_labels.append(b.tolist())\n",
        "\n",
        "print(y_preds.shape)\n",
        "print(f'Accuracy: {accuracy / len(y_pred_list)}')\n",
        "\n",
        "print(y_preds[0])\n",
        "print(y_labels[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0]\n",
            "(200,)\n",
            "Accuracy: 86.5\n",
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y0aPoZjZHXC",
        "outputId": "e744129a-a877-40b7-ccdc-3009e96fe656"
      },
      "source": [
        "print(f'confusion matrix: \\n {confusion_matrix(y_labels,y_preds)}')\n",
        "print(classification_report(y_labels,y_preds))\n",
        "print(f'Accuracy: {accuracy_score(y_labels, y_preds)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix: \n",
            " [[86 14]\n",
            " [13 87]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.86       100\n",
            "           1       0.86      0.87      0.87       100\n",
            "\n",
            "    accuracy                           0.86       200\n",
            "   macro avg       0.87      0.86      0.86       200\n",
            "weighted avg       0.87      0.86      0.86       200\n",
            "\n",
            "Accuracy: 0.865\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}